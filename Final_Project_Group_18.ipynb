{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_links(url: str, keyword: str, start_date: str = \"2020-01\", end_date: str = \"2024-08\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch all Parquet file links from a webpage that match the specified keyword \n",
    "    and fall within the given date range (YYYY-MM format).\n",
    "    \"\"\"\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m\")\n",
    "\n",
    "    response = requests.get(url)  \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)  # Find all <a> tags with href attribute\n",
    "    parquet_links = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link['href'].strip() #Remove extra spaces around href\n",
    "        # Check if the link contains the keyword and ends with .parquet\n",
    "        if keyword in href and href.endswith('.parquet'):\n",
    "            # Extract the date in YYYY-MM format from the file name\n",
    "            date_match = re.search(r'(\\d{4}-\\d{2})', href)\n",
    "            if date_match:\n",
    "                file_date = datetime.strptime(date_match.group(1), \"%Y-%m\")\n",
    "                # Check if the file date falls within the specified range\n",
    "                if start_dt <= file_date <= end_dt:\n",
    "                    parquet_links.append(href)\n",
    "    return parquet_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the Parquet files from the provided links to the specified directory\n",
    "def download_parquet_files(parquet_links: List[str], download_directory: str) -> None:\n",
    "    for idx, file_url in enumerate(parquet_links):\n",
    "        # Generate the complete path for the file\n",
    "        file_name = file_url.split(\"/\")[-1]\n",
    "        file_path = os.path.join(download_directory, file_name)\n",
    "        # If the file already exists, skip the download\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_name} already downloaded, skipping.\")\n",
    "            continue\n",
    "        # Download the file\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {file_name} successfully.\")\n",
    "\n",
    "# Main function to download Parquet data\n",
    "def download_parquet_data(url: str, keyword: str, download_directory: str) -> None:\n",
    "    # Get the links for Parquet files containing the keyword\n",
    "    parquet_links = get_parquet_links(url, keyword,start_date=\"2020-01\", end_date=\"2024-08\")\n",
    "    if parquet_links:\n",
    "        # Download the files that are not already present in the directory\n",
    "        download_parquet_files(parquet_links, download_directory)\n",
    "    else:\n",
    "        print(f\"No matching Parquet files found for keyword: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2024-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-12.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-12.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-12.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-12.parquet already downloaded, skipping.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"  \n",
    "\n",
    "# Download \"Yellow Taxi\" data\n",
    "keyword_yellow_taxi = \"yellow\"\n",
    "download_directory_yellow_taxi = r\"E:\\2024 Fall Academic\\Tools for Analytics\\Project\\Yellow Taxi\"  # Folder to store downloaded files\n",
    "download_parquet_data(url, keyword_yellow_taxi, download_directory_yellow_taxi)\n",
    "\n",
    "\n",
    "# Download \"High Volume For-Hire Vehicle\" data\n",
    "keyword_hvfhv = \"fhvhv\"\n",
    "download_directory_hvfhv = r\"E:\\2024 Fall Academic\\Tools for Analytics\\Project\\HVFHV\"  # Folder to store downloaded files\n",
    "download_parquet_data(url, keyword_hvfhv, download_directory_hvfhv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the sample size using Cochran's formula\n",
    "def calculate_sample_size(population_size: int, e: float = 0.05, p: float = 0.5) -> int:\n",
    "    Z = 1.96  # Z value for 95% confidence level\n",
    "    numerator = Z**2 * p * (1 - p)\n",
    "    denominator = e**2 * (population_size - 1) + Z**2 * p * (1 - p)\n",
    "    sample_size = (numerator / denominator) * population_size\n",
    "    return int(np.ceil(sample_size))  # Round up to ensure enough sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sampling for each Parquet file\n",
    "def sample_parquet_file(file_path: str, output_directory: str) -> None:\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Get the population size (total number of rows in the file)\n",
    "    population_size = len(df)\n",
    "    \n",
    "    # Calculate the required sample size\n",
    "    sample_size = calculate_sample_size(population_size)\n",
    "    print(f\"File: {file_path} - Total records: {population_size}, Sample size: {sample_size}\")\n",
    "    \n",
    "    # Perform random sampling\n",
    "    sampled_data = df.sample(n=sample_size, random_state=40)\n",
    "    \n",
    "    # Save the sampled data to a new file in the output directory\n",
    "    file_name = os.path.basename(file_path)\n",
    "    sampled_file_path = os.path.join(output_directory, f\"sampled_{file_name}\")\n",
    "    sampled_data.to_parquet(sampled_file_path, compression='snappy')\n",
    "    print(f\"Sampled data saved to: {sampled_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to sample all Parquet files in a directory\n",
    "def sample_all_parquet_files(input_directory: str, output_directory: str) -> None:    \n",
    "    # Loop through each file in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        file_path = os.path.join(input_directory, file_name)\n",
    "        \n",
    "        # If the file is already in the output directory, skip the sampling\n",
    "        if os.path.exists(os.path.join(output_directory, f\"sampled_{file_name}\")):\n",
    "            print(f\"File {file_name} already sampled, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Check if the file is a Parquet file\n",
    "        if file_name.endswith('.parquet'):\n",
    "            # Sample the Parquet file and save the result\n",
    "            sample_parquet_file(file_path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sampled_parquet_files(input_directory: str) -> pd.DataFrame:\n",
    "    dataframes = []\n",
    "    # Iterate over all files in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('.parquet'):  # Process only Parquet files\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            # Read the Parquet file into a DataFrame\n",
    "            df = pd.read_parquet(file_path)\n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "    # Concatenate all DataFrames in the list\n",
    "    combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"Successfully merged {len(dataframes)} files into a single DataFrame.\")\n",
    "    return combined_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Sampling yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2020-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-08.parquet already sampled, skipping.\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-08.parquet\n",
      "Successfully merged 56 files into a single DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_24976\\1081269227.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_dataframe = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-21 18:56:18</td>\n",
       "      <td>2020-01-21 19:08:41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-20 15:55:43</td>\n",
       "      <td>2020-01-20 16:00:45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-22 20:05:14</td>\n",
       "      <td>2020-01-22 20:13:53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>162</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.96</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-22 07:04:51</td>\n",
       "      <td>2020-01-22 07:54:32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>159</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>53.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-22 06:58:21</td>\n",
       "      <td>2020-01-22 07:01:47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-08 22:16:46</td>\n",
       "      <td>2024-08-08 22:31:47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-24 18:53:23</td>\n",
       "      <td>2024-08-24 19:11:06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>32.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.09</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21553</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-18 18:21:03</td>\n",
       "      <td>2024-08-18 18:37:13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.68</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21554</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-15 06:34:03</td>\n",
       "      <td>2024-08-15 06:42:49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.48</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21555</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-11 18:58:01</td>\n",
       "      <td>2024-08-11 19:14:18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>88</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21556 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0             2  2020-01-21 18:56:18   2020-01-21 19:08:41              1.0   \n",
       "1             2  2020-01-20 15:55:43   2020-01-20 16:00:45              2.0   \n",
       "2             2  2020-01-22 20:05:14   2020-01-22 20:13:53              1.0   \n",
       "3             1  2020-01-22 07:04:51   2020-01-22 07:54:32              1.0   \n",
       "4             2  2020-01-22 06:58:21   2020-01-22 07:01:47              2.0   \n",
       "...         ...                  ...                   ...              ...   \n",
       "21551         2  2024-08-08 22:16:46   2024-08-08 22:31:47              2.0   \n",
       "21552         2  2024-08-24 18:53:23   2024-08-24 19:11:06              1.0   \n",
       "21553         2  2024-08-18 18:21:03   2024-08-18 18:37:13              1.0   \n",
       "21554         2  2024-08-15 06:34:03   2024-08-15 06:42:49              2.0   \n",
       "21555         2  2024-08-11 18:58:01   2024-08-11 19:14:18              1.0   \n",
       "\n",
       "       trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
       "0               2.27         1.0                  N           170   \n",
       "1               0.87         1.0                  N           164   \n",
       "2               0.76         1.0                  N           162   \n",
       "3               0.00         1.0                  N           159   \n",
       "4               0.75         1.0                  N           100   \n",
       "...              ...         ...                ...           ...   \n",
       "21551           2.37         1.0                  N            68   \n",
       "21552           8.12         1.0                  N           138   \n",
       "21553           1.86         1.0                  N           170   \n",
       "21554           1.64         1.0                  N           161   \n",
       "21555           3.66         1.0                  N            88   \n",
       "\n",
       "       DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0               141             1         10.0    1.0      0.5        1.70   \n",
       "1               170             1          5.5    0.0      0.5        1.00   \n",
       "2               229             1          7.0    0.5      0.5        2.16   \n",
       "3                89             1         46.2    0.0      0.5        0.00   \n",
       "4                50             1          4.5    0.0      0.5        1.95   \n",
       "...             ...           ...          ...    ...      ...         ...   \n",
       "21551           163             2         15.6    1.0      0.5        0.00   \n",
       "21552           263             1         32.4    5.0      0.5        0.00   \n",
       "21553            90             1         14.9    0.0      0.5        3.78   \n",
       "21554           239             1         11.4    0.0      0.5        3.08   \n",
       "21555           246             1         19.8    0.0      0.5        4.00   \n",
       "\n",
       "       tolls_amount  improvement_surcharge  total_amount  \\\n",
       "0              0.00                    0.3         16.00   \n",
       "1              0.00                    0.3          9.80   \n",
       "2              0.00                    0.3         12.96   \n",
       "3              6.12                    0.3         53.12   \n",
       "4              0.00                    0.3          9.75   \n",
       "...             ...                    ...           ...   \n",
       "21551          0.00                    1.0         20.60   \n",
       "21552          6.94                    1.0         50.09   \n",
       "21553          0.00                    1.0         22.68   \n",
       "21554          0.00                    1.0         18.48   \n",
       "21555          0.00                    1.0         27.80   \n",
       "\n",
       "       congestion_surcharge  airport_fee  Airport_fee  \n",
       "0                       2.5          NaN          NaN  \n",
       "1                       2.5          NaN          NaN  \n",
       "2                       2.5          NaN          NaN  \n",
       "3                       0.0          NaN          NaN  \n",
       "4                       2.5          NaN          NaN  \n",
       "...                     ...          ...          ...  \n",
       "21551                   2.5          NaN         0.00  \n",
       "21552                   2.5          NaN         1.75  \n",
       "21553                   2.5          NaN         0.00  \n",
       "21554                   2.5          NaN         0.00  \n",
       "21555                   2.5          NaN         0.00  \n",
       "\n",
       "[21556 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi\"  \n",
    "output_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\" \n",
    "\n",
    "# Perform sampling on all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)\n",
    "# Merge all sampled Parquet files into a single DataFrame\n",
    "yellow_taxi_df = merge_sampled_parquet_files(output_directory)\n",
    "\n",
    "yellow_taxi_df=yellow_taxi_df.dropna(axis=1, how=\"all\")\n",
    "yellow_taxi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "filepath = r\"E:/2024 Fall Academic/Tools for Analytics/Project/yellow_taxi_sampled.csv\"\n",
    "if not os.path.exists(filepath):\n",
    "    # If it doesn't exist, save the dataframe to the file\n",
    "    yellow_taxi_df.to_csv(filepath, index=False)\n",
    "    print(\"File saved successfully.\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Sampling Uber data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File fhvhv_tripdata_2020-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-08.parquet already sampled, skipping.\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-08.parquet\n",
      "Successfully merged 56 files into a single DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_24976\\1081269227.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_dataframe = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>originating_base_num</th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02872</td>\n",
       "      <td>B02872</td>\n",
       "      <td>2020-01-25 06:32:19</td>\n",
       "      <td>2020-01-25 06:38:11</td>\n",
       "      <td>2020-01-25 06:38:42</td>\n",
       "      <td>2020-01-25 06:44:40</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>1.860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.39</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-01-23 01:44:51</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-01-23 01:49:21</td>\n",
       "      <td>2020-01-23 02:12:36</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>9.211</td>\n",
       "      <td>...</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.60</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02872</td>\n",
       "      <td>B02872</td>\n",
       "      <td>2020-01-06 18:16:36</td>\n",
       "      <td>2020-01-06 18:16:51</td>\n",
       "      <td>2020-01-06 18:19:01</td>\n",
       "      <td>2020-01-06 18:29:42</td>\n",
       "      <td>92</td>\n",
       "      <td>53</td>\n",
       "      <td>2.090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.57</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02872</td>\n",
       "      <td>B02872</td>\n",
       "      <td>2020-01-22 21:25:56</td>\n",
       "      <td>2020-01-22 21:28:29</td>\n",
       "      <td>2020-01-22 21:29:23</td>\n",
       "      <td>2020-01-22 21:38:26</td>\n",
       "      <td>161</td>\n",
       "      <td>50</td>\n",
       "      <td>1.520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.14</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-01-24 17:02:22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-01-24 17:05:01</td>\n",
       "      <td>2020-01-24 17:10:08</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>2.289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.39</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21555</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-08-14 19:46:30</td>\n",
       "      <td>2024-08-14 19:51:21</td>\n",
       "      <td>2024-08-14 19:53:23</td>\n",
       "      <td>2024-08-14 20:09:17</td>\n",
       "      <td>188</td>\n",
       "      <td>181</td>\n",
       "      <td>2.370</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.49</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21556</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-08-06 21:14:28</td>\n",
       "      <td>2024-08-06 21:22:51</td>\n",
       "      <td>2024-08-06 21:23:07</td>\n",
       "      <td>2024-08-06 21:37:52</td>\n",
       "      <td>158</td>\n",
       "      <td>13</td>\n",
       "      <td>2.610</td>\n",
       "      <td>...</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>20.40</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21557</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-08-17 13:26:17</td>\n",
       "      <td>2024-08-17 13:27:41</td>\n",
       "      <td>2024-08-17 13:28:59</td>\n",
       "      <td>2024-08-17 13:46:35</td>\n",
       "      <td>82</td>\n",
       "      <td>157</td>\n",
       "      <td>2.640</td>\n",
       "      <td>...</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21558</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B03406</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-31 11:57:25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-08-31 12:21:53</td>\n",
       "      <td>2024-08-31 12:37:53</td>\n",
       "      <td>238</td>\n",
       "      <td>262</td>\n",
       "      <td>2.275</td>\n",
       "      <td>...</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.29</td>\n",
       "      <td>19.20</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21559</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B03406</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-27 18:35:19</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-08-27 18:40:21</td>\n",
       "      <td>2024-08-27 18:57:19</td>\n",
       "      <td>179</td>\n",
       "      <td>138</td>\n",
       "      <td>3.959</td>\n",
       "      <td>...</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.86</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21560 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
       "0                HV0003               B02872               B02872   \n",
       "1                HV0005               B02510                 None   \n",
       "2                HV0003               B02872               B02872   \n",
       "3                HV0003               B02872               B02872   \n",
       "4                HV0005               B02510                 None   \n",
       "...                 ...                  ...                  ...   \n",
       "21555            HV0003               B03404               B03404   \n",
       "21556            HV0003               B03404               B03404   \n",
       "21557            HV0003               B03404               B03404   \n",
       "21558            HV0005               B03406                 None   \n",
       "21559            HV0005               B03406                 None   \n",
       "\n",
       "         request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "0     2020-01-25 06:32:19 2020-01-25 06:38:11 2020-01-25 06:38:42   \n",
       "1     2020-01-23 01:44:51                 NaT 2020-01-23 01:49:21   \n",
       "2     2020-01-06 18:16:36 2020-01-06 18:16:51 2020-01-06 18:19:01   \n",
       "3     2020-01-22 21:25:56 2020-01-22 21:28:29 2020-01-22 21:29:23   \n",
       "4     2020-01-24 17:02:22                 NaT 2020-01-24 17:05:01   \n",
       "...                   ...                 ...                 ...   \n",
       "21555 2024-08-14 19:46:30 2024-08-14 19:51:21 2024-08-14 19:53:23   \n",
       "21556 2024-08-06 21:14:28 2024-08-06 21:22:51 2024-08-06 21:23:07   \n",
       "21557 2024-08-17 13:26:17 2024-08-17 13:27:41 2024-08-17 13:28:59   \n",
       "21558 2024-08-31 11:57:25                 NaT 2024-08-31 12:21:53   \n",
       "21559 2024-08-27 18:35:19                 NaT 2024-08-27 18:40:21   \n",
       "\n",
       "         dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  \\\n",
       "0     2020-01-25 06:44:40            32            18       1.860  ...   \n",
       "1     2020-01-23 02:12:36            88             7       9.211  ...   \n",
       "2     2020-01-06 18:29:42            92            53       2.090  ...   \n",
       "3     2020-01-22 21:38:26           161            50       1.520  ...   \n",
       "4     2020-01-24 17:10:08           132           132       2.289  ...   \n",
       "...                   ...           ...           ...         ...  ...   \n",
       "21555 2024-08-14 20:09:17           188           181       2.370  ...   \n",
       "21556 2024-08-06 21:37:52           158            13       2.610  ...   \n",
       "21557 2024-08-17 13:46:35            82           157       2.640  ...   \n",
       "21558 2024-08-31 12:37:53           238           262       2.275  ...   \n",
       "21559 2024-08-27 18:57:19           179           138       3.959  ...   \n",
       "\n",
       "       sales_tax  congestion_surcharge  airport_fee  tips  driver_pay  \\\n",
       "0           0.50                  0.00          NaN  0.00        5.39   \n",
       "1           2.61                  2.75          NaN  0.00       21.60   \n",
       "2           0.59                  0.00          NaN  0.00        7.57   \n",
       "3           0.77                  2.75          NaN  0.00        6.14   \n",
       "4           0.88                  0.00          NaN  5.00        5.39   \n",
       "...          ...                   ...          ...   ...         ...   \n",
       "21555       1.57                  0.00          0.0  0.00       12.49   \n",
       "21556       3.35                  2.75          0.0  4.49       20.40   \n",
       "21557       1.58                  0.00          0.0  0.00       13.85   \n",
       "21558       2.06                  2.75          0.0  4.29       19.20   \n",
       "21559       1.93                  0.00          2.5  0.00       16.86   \n",
       "\n",
       "       shared_request_flag  shared_match_flag  access_a_ride_flag  \\\n",
       "0                        Y                  N                       \n",
       "1                        N                  N                   N   \n",
       "2                        N                  N                       \n",
       "3                        N                  N                       \n",
       "4                        N                  N                   N   \n",
       "...                    ...                ...                 ...   \n",
       "21555                    N                  N                   N   \n",
       "21556                    N                  N                   N   \n",
       "21557                    N                  N                   N   \n",
       "21558                    N                  N                   N   \n",
       "21559                    N                  N                   N   \n",
       "\n",
       "       wav_request_flag wav_match_flag  \n",
       "0                     N              N  \n",
       "1                     N              N  \n",
       "2                     N              N  \n",
       "3                     N              N  \n",
       "4                     N              N  \n",
       "...                 ...            ...  \n",
       "21555                 N              N  \n",
       "21556                 N              N  \n",
       "21557                 N              N  \n",
       "21558                 N              N  \n",
       "21559                 N              Y  \n",
       "\n",
       "[21560 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV\"  # Folder containing original Parquet files\n",
    "output_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\"  # Folder to save the sampled Parquet files\n",
    "\n",
    "# Perform sampling on all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)\n",
    "# Merge all sampled Parquet files into a single DataFrame\n",
    "HVFHV_df = merge_sampled_parquet_files(output_directory)\n",
    "\n",
    "HVFHV_df=HVFHV_df.dropna(axis=1, how=\"all\")\n",
    "HVFHV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "filepath = r\"E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV_sampled.csv\"\n",
    "if not os.path.exists(filepath):\n",
    "    # If it doesn't exist, save the dataframe to the file\n",
    "    HVFHV_df.to_csv(filepath, index=False)\n",
    "    print(\"File saved successfully.\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cleaning & filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import math\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Loading taxi zones data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zijie\\AppData\\Local\\Temp\\ipykernel_5368\\2223963518.py:24: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones[\"zone_center\"] = zones.geometry.centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>40.691831</td>\n",
       "      <td>-74.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>40.616745</td>\n",
       "      <td>-73.831299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40.864474</td>\n",
       "      <td>-73.847422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.723752</td>\n",
       "      <td>-73.976968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40.552659</td>\n",
       "      <td>-74.188484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID   latitude  longitude\n",
       "0           1  40.691831 -74.174000\n",
       "1           2  40.616745 -73.831299\n",
       "2           3  40.864474 -73.847422\n",
       "3           4  40.723752 -73.976968\n",
       "4           5  40.552659 -74.188484"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\"\n",
    "local_zip = \"taxi_zones.zip\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url, stream=True)\n",
    "with open(local_zip, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"taxi_zones\")\n",
    "\n",
    "# Path to the extracted shapefile\n",
    "shapefile_path = \"taxi_zones/taxi_zones.shp\"\n",
    "\n",
    "zones = gpd.read_file(\"taxi_zones/taxi_zones.shp\")\n",
    "\n",
    "\n",
    "# Reproject to WGS84 (latitude/longitude coordinate system)\n",
    "if zones.crs is not None and zones.crs.to_string() != \"EPSG:4326\":\n",
    "    zones = zones.to_crs(epsg=4326)\n",
    "\n",
    "# Calculate the center of each zone\n",
    "zones[\"zone_center\"] = zones.geometry.centroid\n",
    "zones[\"latitude\"] = zones[\"zone_center\"].y\n",
    "zones[\"longitude\"] = zones[\"zone_center\"].x\n",
    "\n",
    "# Extract only necessary columns\n",
    "zone_centers = zones[[\"LocationID\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "# Check the results\n",
    "zone_centers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Cleaning yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Yellow Taxi data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sampled_data_yellow_taxi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Clean Yellow Taxi data\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning Yellow Taxi data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m cleaned_yellow_taxi \u001b[38;5;241m=\u001b[39m clean_yellow_taxi_data(sampled_data_yellow_taxi, zone_centers)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Save cleaned Yellow Taxi data\u001b[39;00m\n\u001b[0;32m     74\u001b[0m cleaned_yellow_taxi\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow_taxi_cleaned.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sampled_data_yellow_taxi' is not defined"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "def clean_yellow_taxi_data(df: DataFrame, zone_centers: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and filters the Yellow Taxi dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The Yellow Taxi dataset to clean.\n",
    "        zone_centers (pd.DataFrame): DataFrame containing LocationID, latitude, and longitude.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and filtered dataset.\n",
    "    \"\"\"\n",
    "    # Merge pickup and dropoff zones to get latitude and longitude\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"PULocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"pickup_latitude\", \"longitude\": \"pickup_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"DOLocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"dropoff_latitude\", \"longitude\": \"dropoff_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    # Drop rows where coordinates couldn't be determined\n",
    "    df = df.dropna(subset=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "\n",
    "    # Remove rows with invalid trip distance or total amount\n",
    "    df = df[(df[\"trip_distance\"] >= 0) & (df[\"total_amount\"] > 0)]\n",
    "\n",
    "    # Ensure passenger_count is valid (non-negative integers)\n",
    "    df[\"passenger_count\"] = pd.to_numeric(df[\"passenger_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df[df[\"passenger_count\"] >= 0]\n",
    "\n",
    "    # Filter trips within valid latitude/longitude bounding box\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    df = df[\n",
    "        (df[\"pickup_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"pickup_longitude\"].between(LON_MIN, LON_MAX)) &\n",
    "        (df[\"dropoff_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"dropoff_longitude\"].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "\n",
    "    # Normalize column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Ensure datetime columns are in the correct format\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"], errors=\"coerce\")\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop invalid datetime rows\n",
    "    df = df.dropna(subset=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])\n",
    "\n",
    "    # Remove unnecessary columns and keep relevant ones\n",
    "    columns_to_keep = [\n",
    "        \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",  # Times\n",
    "        \"passenger_count\", \"trip_distance\",              # Trip details\n",
    "        \"fare_amount\", \"total_amount\", \"tip_amount\",     # Payment info\n",
    "        \"ratecodeid\", \"store_and_fwd_flag\",              # Taxi-specific info\n",
    "        \"pickup_latitude\", \"pickup_longitude\",           # Pickup coordinates\n",
    "        \"dropoff_latitude\", \"dropoff_longitude\",\n",
    "        \"mta_tax\", \"tolls_amount\", \n",
    "        \"congestion_surcharge\"\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean Yellow Taxi data\n",
    "print(\"Cleaning Yellow Taxi data...\")\n",
    "cleaned_yellow_taxi = clean_yellow_taxi_data(sampled_data_yellow_taxi, zone_centers)\n",
    "\n",
    "# Save cleaned Yellow Taxi data\n",
    "cleaned_yellow_taxi.to_parquet(\"yellow_taxi_cleaned.parquet\")\n",
    "print(\"Cleaned Yellow Taxi data saved: yellow_taxi_cleaned.parquet\")\n",
    "\n",
    "cleaned_yellow_taxi.head()\n",
    "cleaned_yellow_taxi.info()\n",
    "cleaned_yellow_taxi.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Cleaning Uber(HVFHV) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fhv_data(df: DataFrame, zone_centers: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and filters the HVFHV dataset for Uber rides and shared rides.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The FHV dataset to clean.\n",
    "        zone_centers (pd.DataFrame): DataFrame containing LocationID, latitude, and longitude.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and filtered dataset.\n",
    "    \"\"\"\n",
    "    # Merge pickup and dropoff zones to get latitude and longitude\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"PULocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"pickup_latitude\", \"longitude\": \"pickup_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"DOLocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"dropoff_latitude\", \"longitude\": \"dropoff_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    # Drop rows where coordinates couldn't be determined\n",
    "    df = df.dropna(subset=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "    \n",
    "    df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "\n",
    "    # Filter trips within valid latitude/longitude bounding box\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    df = df[\n",
    "        (df[\"pickup_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"pickup_longitude\"].between(LON_MIN, LON_MAX)) &\n",
    "        (df[\"dropoff_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"dropoff_longitude\"].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "\n",
    "    # Normalize column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Ensure datetime columns are in the correct format\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\")\n",
    "    df[\"dropoff_datetime\"] = pd.to_datetime(df[\"dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop invalid datetime rows\n",
    "    df = df.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"])\n",
    "\n",
    "    # Keep relevant columns\n",
    "    columns_to_keep = [\n",
    "        \"pickup_datetime\", \"dropoff_datetime\",            # Times\n",
    "        \"pickup_latitude\", \"pickup_longitude\",            # Pickup coordinates\n",
    "        \"dropoff_latitude\", \"dropoff_longitude\",          # Dropoff coordinates\n",
    "        \"hvfhs_license_num\", \"trip_miles\",\n",
    "        \"base_passenger_fare\", \"tolls\", \"tips\",\t\n",
    "        \"sales_tax\", \"congestion_surcharge\"\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean HVFHV data for Uber rides\n",
    "print(\"Cleaning HVFHV (Uber) data...\")\n",
    "cleaned_fhv = clean_fhv_data(sampled_data_HVFHV, zone_centers)\n",
    "\n",
    "# Save cleaned HVFHV data\n",
    "cleaned_fhv.to_parquet(\"hvfhv_cleaned.parquet\")\n",
    "print(\"Cleaned HVFHV data saved: hvfhv_cleaned.parquet\")\n",
    "\n",
    "cleaned_fhv.head()\n",
    "cleaned_fhv.info()\n",
    "cleaned_fhv.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Loading and cleaning weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"D:\\Columbia University\\24 Fall\\IEOR 4501\\Final Project\\weather_data\"\n",
    "weather_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "dataframes = []\n",
    "for weather_file in weather_files:\n",
    "    file_path = os.path.join(folder_path, weather_file)\n",
    "    df = pd.read_csv(file_path)  \n",
    "    dataframes.append(df) \n",
    "merged_weather_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_weather_df.head()\n",
    "\n",
    "# filepath = r\"D:\\Columbia University\\24 Fall\\IEOR 4501\\Final Project\\merged_weather_df.csv\"\n",
    "# merged_weather_df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for column data types\n",
    "dtype_mapping = {\n",
    "    \"HourlyPrecipitation\": \"string\",\n",
    "    \"DailySnowfall\": \"string\",\n",
    "    \"DailyPrecipitation\": \"string\",\n",
    "    \"LATITUDE\": \"float\",\n",
    "    \"LONGITUDE\": \"float\",\n",
    "}\n",
    "\n",
    "# Load the dataset and specify data types\n",
    "# weather_filepath = r\"D:\\Columbia University\\24 Fall\\IEOR 4501\\Final Project\\merged_weather_df.csv\"\n",
    "weather_data = merged_weather_df\n",
    "\n",
    "# Convert specific columns to numeric types, handling mixed types automatically\n",
    "numeric_columns = [\"HourlyPrecipitation\", \"DailySnowfall\", \"DailyPrecipitation\"]\n",
    "for col in numeric_columns:\n",
    "    # Convert column to string first to handle mixed or non-string values\n",
    "    weather_data[col] = weather_data[col].astype(str)\n",
    "    weather_data[col] = (\n",
    "        weather_data[col]\n",
    "        .str.extract(r'([\\d\\.]+)', expand=False)  # Extract numeric parts\n",
    "        .astype(float)\n",
    "        .fillna(0)  # Fill missing values with 0\n",
    "    )\n",
    "    \n",
    "\n",
    "# Retain the required columns\n",
    "weather_columns_to_keep = [\"REPORT_TYPE\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"HourlyPrecipitation\", \n",
    "                           \"HourlyWindSpeed\", \"DailySnowfall\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\"]\n",
    "weather_data = weather_data[weather_columns_to_keep]\n",
    "\n",
    "# Rename columns to maintain consistency\n",
    "weather_data.rename(columns={\n",
    "    \"REPORT_TYPE\": \"Report_type\",\n",
    "    \"DATE\": \"observation_date\",\n",
    "    \"LATITUDE\": \"geo_latitude\",\n",
    "    \"LONGITUDE\": \"geo_longitude\",\n",
    "    \"HourlyPrecipitation\": \"hourly_precip_mm\",\n",
    "    \"DailyPrecipitation\": \"daily_precip_mm\",\n",
    "    \"HourlyWindSpeed\": \"hourly_wind_mps\",\n",
    "    \"DailyAverageWindSpeed\": \"daily_wind_mps\",\n",
    "    \"DailySnowfall\": \"daily_snowfall_mm\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert the date column to datetime type\n",
    "weather_data['observation_date'] = pd.to_datetime(weather_data['observation_date'])\n",
    "\n",
    "# Fill missing values with 0 (for specific columns only)\n",
    "columns_to_fill = ['hourly_precip_mm', 'daily_precip_mm', \n",
    "                   'hourly_wind_mps', 'daily_wind_mps', 'daily_snowfall_mm']\n",
    "weather_data[columns_to_fill] = weather_data[columns_to_fill].fillna(0)\n",
    "\n",
    "# Ensure the data types of columns are correct\n",
    "weather_data['Report_type'] = weather_data['Report_type'].astype(object)\n",
    "weather_data['hourly_precip_mm'] = weather_data['hourly_precip_mm'].astype(float)\n",
    "weather_data['daily_precip_mm'] = weather_data['daily_precip_mm'].astype(float)\n",
    "weather_data['hourly_wind_mps'] = weather_data['hourly_wind_mps'].astype(float)\n",
    "weather_data['daily_wind_mps'] = weather_data['daily_wind_mps'].astype(float)\n",
    "weather_data['daily_snowfall_mm'] = weather_data['daily_snowfall_mm'].astype(float)\n",
    "weather_data['geo_latitude'] = weather_data['geo_latitude'].astype(float)\n",
    "weather_data['geo_longitude'] = weather_data['geo_longitude'].astype(float)\n",
    "\n",
    "# Save the cleaned data to a file\n",
    "weather_output_file = \"cleaned_weather_data.parquet\"\n",
    "weather_data.to_parquet(weather_output_file, index=False)\n",
    "\n",
    "weather_data = pd.read_parquet(weather_output_file)\n",
    "weather_data.head()\n",
    "weather_data.info()\n",
    "weather_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering hourly weather data\n",
    "weather_data['Report_type'] = weather_data['Report_type'].str.strip()\n",
    "hourly_weather_df = weather_data[weather_data[\"Report_type\"] == \"FM-15\"]\n",
    "hourly_weather_df = hourly_weather_df.dropna(axis=1, how='all')\n",
    "hourly_weather_df.head()\n",
    "# hourly_weather_df.to_csv(\"hourly_weather_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering daily weather data\n",
    "daily_weather_df = weather_data[weather_data[\"Report_type\"] == \"SOD\"]\n",
    "daily_weather_df = daily_weather_df.dropna(axis=1, how='all')\n",
    "daily_weather_df.head()\n",
    "# Save the daily_weather_df to a CSV file\n",
    "# daily_weather_df.to_csv(\"daily_weather_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
