{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_links(url: str, keyword: str, start_date: str = \"2020-01\", end_date: str = \"2024-08\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetch all Parquet file links from a webpage that match the specified keyword \n",
    "    and fall within the given date range (YYYY-MM format).\n",
    "    \"\"\"\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m\")\n",
    "\n",
    "    response = requests.get(url)  \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)  # Find all <a> tags with href attribute\n",
    "    parquet_links = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link['href'].strip() #Remove extra spaces around href\n",
    "        # Check if the link contains the keyword and ends with .parquet\n",
    "        if keyword in href and href.endswith('.parquet'):\n",
    "            # Extract the date in YYYY-MM format from the file name\n",
    "            date_match = re.search(r'(\\d{4}-\\d{2})', href)\n",
    "            if date_match:\n",
    "                file_date = datetime.strptime(date_match.group(1), \"%Y-%m\")\n",
    "                # Check if the file date falls within the specified range\n",
    "                if start_dt <= file_date <= end_dt:\n",
    "                    parquet_links.append(href)\n",
    "    return parquet_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download the Parquet files from the provided links to the specified directory\n",
    "def download_parquet_files(parquet_links: List[str], download_directory: str) -> None:\n",
    "    for idx, file_url in enumerate(parquet_links):\n",
    "        # Generate the complete path for the file\n",
    "        file_name = file_url.split(\"/\")[-1]\n",
    "        file_path = os.path.join(download_directory, file_name)\n",
    "        # If the file already exists, skip the download\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File {file_name} already downloaded, skipping.\")\n",
    "            continue\n",
    "        # Download the file\n",
    "        response = requests.get(file_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {file_name} successfully.\")\n",
    "\n",
    "# Main function to download Parquet data\n",
    "def download_parquet_data(url: str, keyword: str, download_directory: str) -> None:\n",
    "    # Get the links for Parquet files containing the keyword\n",
    "    parquet_links = get_parquet_links(url, keyword,start_date=\"2020-01\", end_date=\"2024-08\")\n",
    "    if parquet_links:\n",
    "        # Download the files that are not already present in the directory\n",
    "        download_parquet_files(parquet_links, download_directory)\n",
    "    else:\n",
    "        print(f\"No matching Parquet files found for keyword: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2024-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2024-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2023-12.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2022-12.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2021-12.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-01.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-02.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-03.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-04.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-05.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-06.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-07.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-08.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-09.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-10.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-11.parquet already downloaded, skipping.\n",
      "File yellow_tripdata_2020-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2024-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2023-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2022-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2021-12.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-01.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-02.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-03.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-04.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-05.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-06.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-07.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-08.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-09.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-10.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-11.parquet already downloaded, skipping.\n",
      "File fhvhv_tripdata_2020-12.parquet already downloaded, skipping.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"  \n",
    "\n",
    "# Download \"Yellow Taxi\" data\n",
    "keyword_yellow_taxi = \"yellow\"\n",
    "download_directory_yellow_taxi = r\"E:\\2024 Fall Academic\\Tools for Analytics\\Project\\Yellow Taxi\"  # Folder to store downloaded files\n",
    "download_parquet_data(url, keyword_yellow_taxi, download_directory_yellow_taxi)\n",
    "\n",
    "\n",
    "# Download \"High Volume For-Hire Vehicle\" data\n",
    "keyword_hvfhv = \"fhvhv\"\n",
    "download_directory_hvfhv = r\"E:\\2024 Fall Academic\\Tools for Analytics\\Project\\HVFHV\"  # Folder to store downloaded files\n",
    "download_parquet_data(url, keyword_hvfhv, download_directory_hvfhv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the sample size using Cochran's formula\n",
    "def calculate_sample_size(population_size: int, e: float = 0.05, p: float = 0.5) -> int:\n",
    "    Z = 1.96  # Z value for 95% confidence level\n",
    "    numerator = Z**2 * p * (1 - p)\n",
    "    denominator = e**2 * (population_size - 1) + Z**2 * p * (1 - p)\n",
    "    sample_size = (numerator / denominator) * population_size\n",
    "    return int(np.ceil(sample_size))  # Round up to ensure enough sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform sampling for each Parquet file\n",
    "def sample_parquet_file(file_path: str, output_directory: str) -> None:\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Get the population size (total number of rows in the file)\n",
    "    population_size = len(df)\n",
    "    \n",
    "    # Calculate the required sample size\n",
    "    sample_size = calculate_sample_size(population_size)\n",
    "    print(f\"File: {file_path} - Total records: {population_size}, Sample size: {sample_size}\")\n",
    "    \n",
    "    # Perform random sampling\n",
    "    sampled_data = df.sample(n=sample_size, random_state=40)\n",
    "    \n",
    "    # Save the sampled data to a new file in the output directory\n",
    "    file_name = os.path.basename(file_path)\n",
    "    sampled_file_path = os.path.join(output_directory, f\"sampled_{file_name}\")\n",
    "    sampled_data.to_parquet(sampled_file_path, compression='snappy')\n",
    "    print(f\"Sampled data saved to: {sampled_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to sample all Parquet files in a directory\n",
    "def sample_all_parquet_files(input_directory: str, output_directory: str) -> None:    \n",
    "    # Loop through each file in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        file_path = os.path.join(input_directory, file_name)\n",
    "        \n",
    "        # If the file is already in the output directory, skip the sampling\n",
    "        if os.path.exists(os.path.join(output_directory, f\"sampled_{file_name}\")):\n",
    "            print(f\"File {file_name} already sampled, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Check if the file is a Parquet file\n",
    "        if file_name.endswith('.parquet'):\n",
    "            # Sample the Parquet file and save the result\n",
    "            sample_parquet_file(file_path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sampled_parquet_files(input_directory: str) -> pd.DataFrame:\n",
    "    dataframes = []\n",
    "    # Iterate over all files in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('.parquet'):  # Process only Parquet files\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            print(f\"Reading file: {file_path}\")\n",
    "            # Read the Parquet file into a DataFrame\n",
    "            df = pd.read_parquet(file_path)\n",
    "            # Append the DataFrame to the list\n",
    "            dataframes.append(df)\n",
    "    # Concatenate all DataFrames in the list\n",
    "    combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"Successfully merged {len(dataframes)} files into a single DataFrame.\")\n",
    "    return combined_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Sampling yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2020-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2020-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2021-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2022-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-08.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-09.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-10.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-11.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2023-12.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-01.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-02.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-03.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-04.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-05.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-06.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-07.parquet already sampled, skipping.\n",
      "File yellow_tripdata_2024-08.parquet already sampled, skipping.\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2020-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2021-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2022-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2023-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\\sampled_yellow_tripdata_2024-08.parquet\n",
      "Successfully merged 56 files into a single DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1081269227.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_dataframe = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-21 18:56:18</td>\n",
       "      <td>2020-01-21 19:08:41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-20 15:55:43</td>\n",
       "      <td>2020-01-20 16:00:45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-22 20:05:14</td>\n",
       "      <td>2020-01-22 20:13:53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>162</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.96</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-22 07:04:51</td>\n",
       "      <td>2020-01-22 07:54:32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>159</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>53.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-22 06:58:21</td>\n",
       "      <td>2020-01-22 07:01:47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-08 22:16:46</td>\n",
       "      <td>2024-08-08 22:31:47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>68</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>15.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-24 18:53:23</td>\n",
       "      <td>2024-08-24 19:11:06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>32.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.94</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.09</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21553</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-18 18:21:03</td>\n",
       "      <td>2024-08-18 18:37:13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>170</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.68</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21554</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-15 06:34:03</td>\n",
       "      <td>2024-08-15 06:42:49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>11.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.48</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21555</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-11 18:58:01</td>\n",
       "      <td>2024-08-11 19:14:18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>88</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21556 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0             2  2020-01-21 18:56:18   2020-01-21 19:08:41              1.0   \n",
       "1             2  2020-01-20 15:55:43   2020-01-20 16:00:45              2.0   \n",
       "2             2  2020-01-22 20:05:14   2020-01-22 20:13:53              1.0   \n",
       "3             1  2020-01-22 07:04:51   2020-01-22 07:54:32              1.0   \n",
       "4             2  2020-01-22 06:58:21   2020-01-22 07:01:47              2.0   \n",
       "...         ...                  ...                   ...              ...   \n",
       "21551         2  2024-08-08 22:16:46   2024-08-08 22:31:47              2.0   \n",
       "21552         2  2024-08-24 18:53:23   2024-08-24 19:11:06              1.0   \n",
       "21553         2  2024-08-18 18:21:03   2024-08-18 18:37:13              1.0   \n",
       "21554         2  2024-08-15 06:34:03   2024-08-15 06:42:49              2.0   \n",
       "21555         2  2024-08-11 18:58:01   2024-08-11 19:14:18              1.0   \n",
       "\n",
       "       trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
       "0               2.27         1.0                  N           170   \n",
       "1               0.87         1.0                  N           164   \n",
       "2               0.76         1.0                  N           162   \n",
       "3               0.00         1.0                  N           159   \n",
       "4               0.75         1.0                  N           100   \n",
       "...              ...         ...                ...           ...   \n",
       "21551           2.37         1.0                  N            68   \n",
       "21552           8.12         1.0                  N           138   \n",
       "21553           1.86         1.0                  N           170   \n",
       "21554           1.64         1.0                  N           161   \n",
       "21555           3.66         1.0                  N            88   \n",
       "\n",
       "       DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "0               141             1         10.0    1.0      0.5        1.70   \n",
       "1               170             1          5.5    0.0      0.5        1.00   \n",
       "2               229             1          7.0    0.5      0.5        2.16   \n",
       "3                89             1         46.2    0.0      0.5        0.00   \n",
       "4                50             1          4.5    0.0      0.5        1.95   \n",
       "...             ...           ...          ...    ...      ...         ...   \n",
       "21551           163             2         15.6    1.0      0.5        0.00   \n",
       "21552           263             1         32.4    5.0      0.5        0.00   \n",
       "21553            90             1         14.9    0.0      0.5        3.78   \n",
       "21554           239             1         11.4    0.0      0.5        3.08   \n",
       "21555           246             1         19.8    0.0      0.5        4.00   \n",
       "\n",
       "       tolls_amount  improvement_surcharge  total_amount  \\\n",
       "0              0.00                    0.3         16.00   \n",
       "1              0.00                    0.3          9.80   \n",
       "2              0.00                    0.3         12.96   \n",
       "3              6.12                    0.3         53.12   \n",
       "4              0.00                    0.3          9.75   \n",
       "...             ...                    ...           ...   \n",
       "21551          0.00                    1.0         20.60   \n",
       "21552          6.94                    1.0         50.09   \n",
       "21553          0.00                    1.0         22.68   \n",
       "21554          0.00                    1.0         18.48   \n",
       "21555          0.00                    1.0         27.80   \n",
       "\n",
       "       congestion_surcharge  airport_fee  Airport_fee  \n",
       "0                       2.5          NaN          NaN  \n",
       "1                       2.5          NaN          NaN  \n",
       "2                       2.5          NaN          NaN  \n",
       "3                       0.0          NaN          NaN  \n",
       "4                       2.5          NaN          NaN  \n",
       "...                     ...          ...          ...  \n",
       "21551                   2.5          NaN         0.00  \n",
       "21552                   2.5          NaN         1.75  \n",
       "21553                   2.5          NaN         0.00  \n",
       "21554                   2.5          NaN         0.00  \n",
       "21555                   2.5          NaN         0.00  \n",
       "\n",
       "[21556 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi\"  \n",
    "output_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/Yellow Taxi Sample\" \n",
    "\n",
    "# Perform sampling on all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)\n",
    "# Merge all sampled Parquet files into a single DataFrame\n",
    "yellow_taxi_df = merge_sampled_parquet_files(output_directory)\n",
    "\n",
    "yellow_taxi_df=yellow_taxi_df.dropna(axis=1, how=\"all\")\n",
    "yellow_taxi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "filepath = r\"E:/2024 Fall Academic/Tools for Analytics/Project/yellow_taxi_sampled.csv\"\n",
    "if not os.path.exists(filepath):\n",
    "    # If it doesn't exist, save the dataframe to the file\n",
    "    yellow_taxi_df.to_csv(filepath, index=False)\n",
    "    print(\"File saved successfully.\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Sampling Uber data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File fhvhv_tripdata_2020-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2020-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2021-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2022-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-08.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-09.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-10.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-11.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2023-12.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-01.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-02.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-03.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-04.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-05.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-06.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-07.parquet already sampled, skipping.\n",
      "File fhvhv_tripdata_2024-08.parquet already sampled, skipping.\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2020-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2021-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2022-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-08.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-09.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-10.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-11.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2023-12.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-01.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-02.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-03.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-04.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-05.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-06.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-07.parquet\n",
      "Reading file: E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\\sampled_fhvhv_tripdata_2024-08.parquet\n",
      "Successfully merged 56 files into a single DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1081269227.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_dataframe = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>originating_base_num</th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02872</td>\n",
       "      <td>B02872</td>\n",
       "      <td>2020-01-25 06:32:19</td>\n",
       "      <td>2020-01-25 06:38:11</td>\n",
       "      <td>2020-01-25 06:38:42</td>\n",
       "      <td>2020-01-25 06:44:40</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>1.860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.39</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-01-23 01:44:51</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-01-23 01:49:21</td>\n",
       "      <td>2020-01-23 02:12:36</td>\n",
       "      <td>88</td>\n",
       "      <td>7</td>\n",
       "      <td>9.211</td>\n",
       "      <td>...</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.60</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02872</td>\n",
       "      <td>B02872</td>\n",
       "      <td>2020-01-06 18:16:36</td>\n",
       "      <td>2020-01-06 18:16:51</td>\n",
       "      <td>2020-01-06 18:19:01</td>\n",
       "      <td>2020-01-06 18:29:42</td>\n",
       "      <td>92</td>\n",
       "      <td>53</td>\n",
       "      <td>2.090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.57</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02872</td>\n",
       "      <td>B02872</td>\n",
       "      <td>2020-01-22 21:25:56</td>\n",
       "      <td>2020-01-22 21:28:29</td>\n",
       "      <td>2020-01-22 21:29:23</td>\n",
       "      <td>2020-01-22 21:38:26</td>\n",
       "      <td>161</td>\n",
       "      <td>50</td>\n",
       "      <td>1.520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>2.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.14</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B02510</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-01-24 17:02:22</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-01-24 17:05:01</td>\n",
       "      <td>2020-01-24 17:10:08</td>\n",
       "      <td>132</td>\n",
       "      <td>132</td>\n",
       "      <td>2.289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.39</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21555</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-08-14 19:46:30</td>\n",
       "      <td>2024-08-14 19:51:21</td>\n",
       "      <td>2024-08-14 19:53:23</td>\n",
       "      <td>2024-08-14 20:09:17</td>\n",
       "      <td>188</td>\n",
       "      <td>181</td>\n",
       "      <td>2.370</td>\n",
       "      <td>...</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.49</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21556</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-08-06 21:14:28</td>\n",
       "      <td>2024-08-06 21:22:51</td>\n",
       "      <td>2024-08-06 21:23:07</td>\n",
       "      <td>2024-08-06 21:37:52</td>\n",
       "      <td>158</td>\n",
       "      <td>13</td>\n",
       "      <td>2.610</td>\n",
       "      <td>...</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>20.40</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21557</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B03404</td>\n",
       "      <td>B03404</td>\n",
       "      <td>2024-08-17 13:26:17</td>\n",
       "      <td>2024-08-17 13:27:41</td>\n",
       "      <td>2024-08-17 13:28:59</td>\n",
       "      <td>2024-08-17 13:46:35</td>\n",
       "      <td>82</td>\n",
       "      <td>157</td>\n",
       "      <td>2.640</td>\n",
       "      <td>...</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.85</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21558</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B03406</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-31 11:57:25</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-08-31 12:21:53</td>\n",
       "      <td>2024-08-31 12:37:53</td>\n",
       "      <td>238</td>\n",
       "      <td>262</td>\n",
       "      <td>2.275</td>\n",
       "      <td>...</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.29</td>\n",
       "      <td>19.20</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21559</th>\n",
       "      <td>HV0005</td>\n",
       "      <td>B03406</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-27 18:35:19</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-08-27 18:40:21</td>\n",
       "      <td>2024-08-27 18:57:19</td>\n",
       "      <td>179</td>\n",
       "      <td>138</td>\n",
       "      <td>3.959</td>\n",
       "      <td>...</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.86</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21560 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
       "0                HV0003               B02872               B02872   \n",
       "1                HV0005               B02510                 None   \n",
       "2                HV0003               B02872               B02872   \n",
       "3                HV0003               B02872               B02872   \n",
       "4                HV0005               B02510                 None   \n",
       "...                 ...                  ...                  ...   \n",
       "21555            HV0003               B03404               B03404   \n",
       "21556            HV0003               B03404               B03404   \n",
       "21557            HV0003               B03404               B03404   \n",
       "21558            HV0005               B03406                 None   \n",
       "21559            HV0005               B03406                 None   \n",
       "\n",
       "         request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "0     2020-01-25 06:32:19 2020-01-25 06:38:11 2020-01-25 06:38:42   \n",
       "1     2020-01-23 01:44:51                 NaT 2020-01-23 01:49:21   \n",
       "2     2020-01-06 18:16:36 2020-01-06 18:16:51 2020-01-06 18:19:01   \n",
       "3     2020-01-22 21:25:56 2020-01-22 21:28:29 2020-01-22 21:29:23   \n",
       "4     2020-01-24 17:02:22                 NaT 2020-01-24 17:05:01   \n",
       "...                   ...                 ...                 ...   \n",
       "21555 2024-08-14 19:46:30 2024-08-14 19:51:21 2024-08-14 19:53:23   \n",
       "21556 2024-08-06 21:14:28 2024-08-06 21:22:51 2024-08-06 21:23:07   \n",
       "21557 2024-08-17 13:26:17 2024-08-17 13:27:41 2024-08-17 13:28:59   \n",
       "21558 2024-08-31 11:57:25                 NaT 2024-08-31 12:21:53   \n",
       "21559 2024-08-27 18:35:19                 NaT 2024-08-27 18:40:21   \n",
       "\n",
       "         dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  \\\n",
       "0     2020-01-25 06:44:40            32            18       1.860  ...   \n",
       "1     2020-01-23 02:12:36            88             7       9.211  ...   \n",
       "2     2020-01-06 18:29:42            92            53       2.090  ...   \n",
       "3     2020-01-22 21:38:26           161            50       1.520  ...   \n",
       "4     2020-01-24 17:10:08           132           132       2.289  ...   \n",
       "...                   ...           ...           ...         ...  ...   \n",
       "21555 2024-08-14 20:09:17           188           181       2.370  ...   \n",
       "21556 2024-08-06 21:37:52           158            13       2.610  ...   \n",
       "21557 2024-08-17 13:46:35            82           157       2.640  ...   \n",
       "21558 2024-08-31 12:37:53           238           262       2.275  ...   \n",
       "21559 2024-08-27 18:57:19           179           138       3.959  ...   \n",
       "\n",
       "       sales_tax  congestion_surcharge  airport_fee  tips  driver_pay  \\\n",
       "0           0.50                  0.00          NaN  0.00        5.39   \n",
       "1           2.61                  2.75          NaN  0.00       21.60   \n",
       "2           0.59                  0.00          NaN  0.00        7.57   \n",
       "3           0.77                  2.75          NaN  0.00        6.14   \n",
       "4           0.88                  0.00          NaN  5.00        5.39   \n",
       "...          ...                   ...          ...   ...         ...   \n",
       "21555       1.57                  0.00          0.0  0.00       12.49   \n",
       "21556       3.35                  2.75          0.0  4.49       20.40   \n",
       "21557       1.58                  0.00          0.0  0.00       13.85   \n",
       "21558       2.06                  2.75          0.0  4.29       19.20   \n",
       "21559       1.93                  0.00          2.5  0.00       16.86   \n",
       "\n",
       "       shared_request_flag  shared_match_flag  access_a_ride_flag  \\\n",
       "0                        Y                  N                       \n",
       "1                        N                  N                   N   \n",
       "2                        N                  N                       \n",
       "3                        N                  N                       \n",
       "4                        N                  N                   N   \n",
       "...                    ...                ...                 ...   \n",
       "21555                    N                  N                   N   \n",
       "21556                    N                  N                   N   \n",
       "21557                    N                  N                   N   \n",
       "21558                    N                  N                   N   \n",
       "21559                    N                  N                   N   \n",
       "\n",
       "       wav_request_flag wav_match_flag  \n",
       "0                     N              N  \n",
       "1                     N              N  \n",
       "2                     N              N  \n",
       "3                     N              N  \n",
       "4                     N              N  \n",
       "...                 ...            ...  \n",
       "21555                 N              N  \n",
       "21556                 N              N  \n",
       "21557                 N              N  \n",
       "21558                 N              N  \n",
       "21559                 N              Y  \n",
       "\n",
       "[21560 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV\"  # Folder containing original Parquet files\n",
    "output_directory = \"E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV Sample\"  # Folder to save the sampled Parquet files\n",
    "\n",
    "# Perform sampling on all Parquet files in the input directory\n",
    "sample_all_parquet_files(input_directory, output_directory)\n",
    "# Merge all sampled Parquet files into a single DataFrame\n",
    "HVFHV_df = merge_sampled_parquet_files(output_directory)\n",
    "\n",
    "HVFHV_df=HVFHV_df.dropna(axis=1, how=\"all\")\n",
    "HVFHV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "filepath = r\"E:/2024 Fall Academic/Tools for Analytics/Project/HVFHV_sampled.csv\"\n",
    "if not os.path.exists(filepath):\n",
    "    # If it doesn't exist, save the dataframe to the file\n",
    "    HVFHV_df.to_csv(filepath, index=False)\n",
    "    print(\"File saved successfully.\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping save.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cleaning & filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import math\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Loading yellow taxi and hvfhv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_yellow_taxi = yellow_taxi_df\n",
    "sampled_data_HVFHV = HVFHV_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Loading taxi zones data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\2223963518.py:24: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  zones[\"zone_center\"] = zones.geometry.centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>40.691831</td>\n",
       "      <td>-74.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>40.616745</td>\n",
       "      <td>-73.831299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40.864473</td>\n",
       "      <td>-73.847422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.723752</td>\n",
       "      <td>-73.976968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40.552659</td>\n",
       "      <td>-74.188485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID   latitude  longitude\n",
       "0           1  40.691831 -74.174000\n",
       "1           2  40.616745 -73.831299\n",
       "2           3  40.864473 -73.847422\n",
       "3           4  40.723752 -73.976968\n",
       "4           5  40.552659 -74.188485"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip\"\n",
    "local_zip = \"taxi_zones.zip\"\n",
    "\n",
    "# Download the ZIP file\n",
    "response = requests.get(url, stream=True)\n",
    "with open(local_zip, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"taxi_zones\")\n",
    "\n",
    "# Path to the extracted shapefile\n",
    "shapefile_path = \"taxi_zones/taxi_zones.shp\"\n",
    "\n",
    "zones = gpd.read_file(\"taxi_zones/taxi_zones.shp\")\n",
    "\n",
    "\n",
    "# Reproject to WGS84 (latitude/longitude coordinate system)\n",
    "if zones.crs is not None and zones.crs.to_string() != \"EPSG:4326\":\n",
    "    zones = zones.to_crs(epsg=4326)\n",
    "\n",
    "# Calculate the center of each zone\n",
    "zones[\"zone_center\"] = zones.geometry.centroid\n",
    "zones[\"latitude\"] = zones[\"zone_center\"].y\n",
    "zones[\"longitude\"] = zones[\"zone_center\"].x\n",
    "\n",
    "# Extract only necessary columns\n",
    "zone_centers = zones[[\"LocationID\", \"latitude\", \"longitude\"]]\n",
    "\n",
    "# Check the results\n",
    "zone_centers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Cleaning yellow taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Yellow Taxi data...\n",
      "Cleaned Yellow Taxi data saved: yellow_taxi_cleaned.parquet\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21049 entries, 0 to 21559\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   tpep_pickup_datetime   21049 non-null  datetime64[us]\n",
      " 1   tpep_dropoff_datetime  21049 non-null  datetime64[us]\n",
      " 2   passenger_count        21049 non-null  int32         \n",
      " 3   trip_distance          21049 non-null  float64       \n",
      " 4   fare_amount            21049 non-null  float64       \n",
      " 5   total_amount           21049 non-null  float64       \n",
      " 6   tip_amount             21049 non-null  float64       \n",
      " 7   ratecodeid             19893 non-null  float64       \n",
      " 8   store_and_fwd_flag     19893 non-null  object        \n",
      " 9   pickup_latitude        21049 non-null  float64       \n",
      " 10  pickup_longitude       21049 non-null  float64       \n",
      " 11  dropoff_latitude       21049 non-null  float64       \n",
      " 12  dropoff_longitude      21049 non-null  float64       \n",
      " 13  mta_tax                21049 non-null  float64       \n",
      " 14  tolls_amount           21049 non-null  float64       \n",
      " 15  congestion_surcharge   19893 non-null  float64       \n",
      "dtypes: datetime64[us](2), float64(12), int32(1), object(1)\n",
      "memory usage: 2.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>ratecodeid</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21049</td>\n",
       "      <td>21049</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>19893.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>21049.000000</td>\n",
       "      <td>19893.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-04-30 07:06:19.554705</td>\n",
       "      <td>2022-04-30 07:22:25.957432</td>\n",
       "      <td>1.337308</td>\n",
       "      <td>3.175365</td>\n",
       "      <td>15.361859</td>\n",
       "      <td>22.396931</td>\n",
       "      <td>2.654193</td>\n",
       "      <td>1.450862</td>\n",
       "      <td>40.753791</td>\n",
       "      <td>-73.967146</td>\n",
       "      <td>40.755399</td>\n",
       "      <td>-73.970248</td>\n",
       "      <td>0.497022</td>\n",
       "      <td>0.420866</td>\n",
       "      <td>2.311366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:24:53</td>\n",
       "      <td>2020-01-01 00:27:48</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-26 18:30:07</td>\n",
       "      <td>2021-02-26 18:39:46</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.740438</td>\n",
       "      <td>-73.989844</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989844</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-04-28 14:40:09</td>\n",
       "      <td>2022-04-28 15:08:55</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.758027</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758027</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-06-29 20:13:20</td>\n",
       "      <td>2023-06-29 20:24:53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.290000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>24.440000</td>\n",
       "      <td>3.440000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.961763</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 23:07:46</td>\n",
       "      <td>2024-08-31 23:15:05</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>43.920000</td>\n",
       "      <td>155.300000</td>\n",
       "      <td>220.690000</td>\n",
       "      <td>82.690000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726656</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726656</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>30.050000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.032057</td>\n",
       "      <td>3.959389</td>\n",
       "      <td>13.669837</td>\n",
       "      <td>17.242320</td>\n",
       "      <td>3.152100</td>\n",
       "      <td>6.283707</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>0.044453</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.036825</td>\n",
       "      <td>0.045027</td>\n",
       "      <td>1.751237</td>\n",
       "      <td>0.660322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
       "count                       21049                       21049   \n",
       "mean   2022-04-30 07:06:19.554705  2022-04-30 07:22:25.957432   \n",
       "min           2020-01-01 00:24:53         2020-01-01 00:27:48   \n",
       "25%           2021-02-26 18:30:07         2021-02-26 18:39:46   \n",
       "50%           2022-04-28 14:40:09         2022-04-28 15:08:55   \n",
       "75%           2023-06-29 20:13:20         2023-06-29 20:24:53   \n",
       "max           2024-08-31 23:07:46         2024-08-31 23:15:05   \n",
       "std                           NaN                         NaN   \n",
       "\n",
       "       passenger_count  trip_distance   fare_amount  total_amount  \\\n",
       "count     21049.000000   21049.000000  21049.000000  21049.000000   \n",
       "mean          1.337308       3.175365     15.361859     22.396931   \n",
       "min           0.000000       0.000000     -3.000000      0.300000   \n",
       "25%           1.000000       1.060000      7.500000     12.700000   \n",
       "50%           1.000000       1.800000     10.700000     16.800000   \n",
       "75%           1.000000       3.290000     17.500000     24.440000   \n",
       "max           8.000000      43.920000    155.300000    220.690000   \n",
       "std           1.032057       3.959389     13.669837     17.242320   \n",
       "\n",
       "         tip_amount    ratecodeid  pickup_latitude  pickup_longitude  \\\n",
       "count  21049.000000  19893.000000     21049.000000      21049.000000   \n",
       "mean       2.654193      1.450862        40.753791        -73.967146   \n",
       "min        0.000000      1.000000        40.576961        -74.174000   \n",
       "25%        0.000000      1.000000        40.740438        -73.989844   \n",
       "50%        2.100000      1.000000        40.758027        -73.977698   \n",
       "75%        3.440000      1.000000        40.773633        -73.961763   \n",
       "max       82.690000     99.000000        40.899529        -73.726656   \n",
       "std        3.152100      6.283707         0.032020          0.044453   \n",
       "\n",
       "       dropoff_latitude  dropoff_longitude       mta_tax  tolls_amount  \\\n",
       "count      21049.000000       21049.000000  21049.000000  21049.000000   \n",
       "mean          40.755399         -73.970248      0.497022      0.420866   \n",
       "min           40.576961         -74.174000      0.000000      0.000000   \n",
       "25%           40.740337         -73.989844      0.500000      0.000000   \n",
       "50%           40.758027         -73.977698      0.500000      0.000000   \n",
       "75%           40.775932         -73.959635      0.500000      0.000000   \n",
       "max           40.899529         -73.726656      2.780000     30.050000   \n",
       "std            0.033613           0.036825      0.045027      1.751237   \n",
       "\n",
       "       congestion_surcharge  \n",
       "count          19893.000000  \n",
       "mean               2.311366  \n",
       "min                0.000000  \n",
       "25%                2.500000  \n",
       "50%                2.500000  \n",
       "75%                2.500000  \n",
       "max                2.500000  \n",
       "std                0.660322  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "def clean_yellow_taxi_data(df: DataFrame, zone_centers: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and filters the Yellow Taxi dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The Yellow Taxi dataset to clean.\n",
    "        zone_centers (pd.DataFrame): DataFrame containing LocationID, latitude, and longitude.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and filtered dataset.\n",
    "    \"\"\"\n",
    "    # Merge pickup and dropoff zones to get latitude and longitude\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"PULocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"pickup_latitude\", \"longitude\": \"pickup_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"DOLocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"dropoff_latitude\", \"longitude\": \"dropoff_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    # Drop rows where coordinates couldn't be determined\n",
    "    df = df.dropna(subset=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "\n",
    "    # Remove rows with invalid trip distance or total amount\n",
    "    df = df[(df[\"trip_distance\"] >= 0) & (df[\"total_amount\"] > 0)]\n",
    "\n",
    "    # Ensure passenger_count is valid (non-negative integers)\n",
    "    df[\"passenger_count\"] = pd.to_numeric(df[\"passenger_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    df = df[df[\"passenger_count\"] >= 0]\n",
    "\n",
    "    # Filter trips within valid latitude/longitude bounding box\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    df = df[\n",
    "        (df[\"pickup_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"pickup_longitude\"].between(LON_MIN, LON_MAX)) &\n",
    "        (df[\"dropoff_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"dropoff_longitude\"].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "\n",
    "    # Normalize column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Ensure datetime columns are in the correct format\n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"], errors=\"coerce\")\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop invalid datetime rows\n",
    "    df = df.dropna(subset=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])\n",
    "\n",
    "    # Remove unnecessary columns and keep relevant ones\n",
    "    columns_to_keep = [\n",
    "        \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",  # Times\n",
    "        \"passenger_count\", \"trip_distance\",              # Trip details\n",
    "        \"fare_amount\", \"total_amount\", \"tip_amount\",     # Payment info\n",
    "        \"ratecodeid\", \"store_and_fwd_flag\",              # Taxi-specific info\n",
    "        \"pickup_latitude\", \"pickup_longitude\",           # Pickup coordinates\n",
    "        \"dropoff_latitude\", \"dropoff_longitude\",\n",
    "        \"mta_tax\", \"tolls_amount\", \n",
    "        \"congestion_surcharge\"\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean Yellow Taxi data\n",
    "print(\"Cleaning Yellow Taxi data...\")\n",
    "cleaned_yellow_taxi = clean_yellow_taxi_data(sampled_data_yellow_taxi, zone_centers)\n",
    "\n",
    "# Save cleaned Yellow Taxi data\n",
    "cleaned_yellow_taxi.to_parquet(\"yellow_taxi_cleaned.parquet\")\n",
    "print(\"Cleaned Yellow Taxi data saved: yellow_taxi_cleaned.parquet\")\n",
    "\n",
    "cleaned_yellow_taxi.head()\n",
    "cleaned_yellow_taxi.info()\n",
    "cleaned_yellow_taxi.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Cleaning Uber(HVFHV) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning HVFHV (Uber) data...\n",
      "Cleaned HVFHV data saved: hvfhv_cleaned.parquet\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15148 entries, 0 to 21673\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   pickup_datetime       15148 non-null  datetime64[us]\n",
      " 1   dropoff_datetime      15148 non-null  datetime64[us]\n",
      " 2   pickup_latitude       15148 non-null  float64       \n",
      " 3   pickup_longitude      15148 non-null  float64       \n",
      " 4   dropoff_latitude      15148 non-null  float64       \n",
      " 5   dropoff_longitude     15148 non-null  float64       \n",
      " 6   hvfhs_license_num     15148 non-null  object        \n",
      " 7   trip_miles            15148 non-null  float64       \n",
      " 8   base_passenger_fare   15148 non-null  float64       \n",
      " 9   tolls                 15148 non-null  float64       \n",
      " 10  tips                  15148 non-null  float64       \n",
      " 11  sales_tax             15148 non-null  float64       \n",
      " 12  congestion_surcharge  15148 non-null  float64       \n",
      "dtypes: datetime64[us](2), float64(10), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>tips</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15148</td>\n",
       "      <td>15148</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "      <td>15148.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-05 02:30:53.698838</td>\n",
       "      <td>2022-05-05 02:48:56.319381</td>\n",
       "      <td>40.738324</td>\n",
       "      <td>-73.934216</td>\n",
       "      <td>40.737299</td>\n",
       "      <td>-73.934927</td>\n",
       "      <td>4.482942</td>\n",
       "      <td>21.314437</td>\n",
       "      <td>0.709385</td>\n",
       "      <td>0.821847</td>\n",
       "      <td>1.900220</td>\n",
       "      <td>1.058622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 01:55:22</td>\n",
       "      <td>2020-01-01 02:01:15</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.170887</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-28 15:44:36.500000</td>\n",
       "      <td>2021-02-28 16:03:54</td>\n",
       "      <td>40.691507</td>\n",
       "      <td>-73.984196</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>10.660000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-06 05:54:46.500000</td>\n",
       "      <td>2022-05-06 06:11:44</td>\n",
       "      <td>40.739495</td>\n",
       "      <td>-73.948136</td>\n",
       "      <td>40.739495</td>\n",
       "      <td>-73.947442</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>16.910000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-04 22:05:50</td>\n",
       "      <td>2023-07-04 22:33:19.250000</td>\n",
       "      <td>40.774375</td>\n",
       "      <td>-73.899735</td>\n",
       "      <td>40.774375</td>\n",
       "      <td>-73.898956</td>\n",
       "      <td>5.730000</td>\n",
       "      <td>26.590000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.382500</td>\n",
       "      <td>2.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 23:11:01</td>\n",
       "      <td>2024-08-31 23:20:16</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726656</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726656</td>\n",
       "      <td>51.120000</td>\n",
       "      <td>190.230000</td>\n",
       "      <td>46.450000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068337</td>\n",
       "      <td>0.064469</td>\n",
       "      <td>0.068941</td>\n",
       "      <td>0.067936</td>\n",
       "      <td>4.371194</td>\n",
       "      <td>15.647353</td>\n",
       "      <td>2.755491</td>\n",
       "      <td>2.626176</td>\n",
       "      <td>1.439267</td>\n",
       "      <td>1.333595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pickup_datetime            dropoff_datetime  \\\n",
       "count                       15148                       15148   \n",
       "mean   2022-05-05 02:30:53.698838  2022-05-05 02:48:56.319381   \n",
       "min           2020-01-01 01:55:22         2020-01-01 02:01:15   \n",
       "25%    2021-02-28 15:44:36.500000         2021-02-28 16:03:54   \n",
       "50%    2022-05-06 05:54:46.500000         2022-05-06 06:11:44   \n",
       "75%           2023-07-04 22:05:50  2023-07-04 22:33:19.250000   \n",
       "max           2024-08-31 23:11:01         2024-08-31 23:20:16   \n",
       "std                           NaN                         NaN   \n",
       "\n",
       "       pickup_latitude  pickup_longitude  dropoff_latitude  dropoff_longitude  \\\n",
       "count     15148.000000      15148.000000      15148.000000       15148.000000   \n",
       "mean         40.738324        -73.934216         40.737299         -73.934927   \n",
       "min          40.561994        -74.170887         40.561994         -74.174000   \n",
       "25%          40.691507        -73.984196         40.691201         -73.984052   \n",
       "50%          40.739495        -73.948136         40.739495         -73.947442   \n",
       "75%          40.774375        -73.899735         40.774375         -73.898956   \n",
       "max          40.899529        -73.726656         40.899529         -73.726656   \n",
       "std           0.068337          0.064469          0.068941           0.067936   \n",
       "\n",
       "         trip_miles  base_passenger_fare         tolls          tips  \\\n",
       "count  15148.000000         15148.000000  15148.000000  15148.000000   \n",
       "mean       4.482942            21.314437      0.709385      0.821847   \n",
       "min        0.000000            -8.100000      0.000000      0.000000   \n",
       "25%        1.570000            10.660000      0.000000      0.000000   \n",
       "50%        2.860000            16.910000      0.000000      0.000000   \n",
       "75%        5.730000            26.590000      0.000000      0.000000   \n",
       "max       51.120000           190.230000     46.450000    100.000000   \n",
       "std        4.371194            15.647353      2.755491      2.626176   \n",
       "\n",
       "          sales_tax  congestion_surcharge  \n",
       "count  15148.000000          15148.000000  \n",
       "mean       1.900220              1.058622  \n",
       "min        0.000000              0.000000  \n",
       "25%        0.930000              0.000000  \n",
       "50%        1.480000              0.000000  \n",
       "75%        2.382500              2.750000  \n",
       "max       15.900000              5.500000  \n",
       "std        1.439267              1.333595  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_fhv_data(df: DataFrame, zone_centers: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and filters the HVFHV dataset for Uber rides and shared rides.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The FHV dataset to clean.\n",
    "        zone_centers (pd.DataFrame): DataFrame containing LocationID, latitude, and longitude.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned and filtered dataset.\n",
    "    \"\"\"\n",
    "    # Merge pickup and dropoff zones to get latitude and longitude\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"PULocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"pickup_latitude\", \"longitude\": \"pickup_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    df = df.merge(zone_centers, how=\"left\", left_on=\"DOLocationID\", right_on=\"LocationID\")\n",
    "    df.rename(columns={\"latitude\": \"dropoff_latitude\", \"longitude\": \"dropoff_longitude\"}, inplace=True)\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    # Drop rows where coordinates couldn't be determined\n",
    "    df = df.dropna(subset=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n",
    "    \n",
    "    df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "\n",
    "    # Filter trips within valid latitude/longitude bounding box\n",
    "    LAT_MIN, LAT_MAX = 40.560445, 40.908524\n",
    "    LON_MIN, LON_MAX = -74.242330, -73.717047\n",
    "    df = df[\n",
    "        (df[\"pickup_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"pickup_longitude\"].between(LON_MIN, LON_MAX)) &\n",
    "        (df[\"dropoff_latitude\"].between(LAT_MIN, LAT_MAX)) &\n",
    "        (df[\"dropoff_longitude\"].between(LON_MIN, LON_MAX))\n",
    "    ]\n",
    "\n",
    "    # Normalize column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Ensure datetime columns are in the correct format\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"], errors=\"coerce\")\n",
    "    df[\"dropoff_datetime\"] = pd.to_datetime(df[\"dropoff_datetime\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop invalid datetime rows\n",
    "    df = df.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"])\n",
    "\n",
    "    # Keep relevant columns\n",
    "    columns_to_keep = [\n",
    "        \"pickup_datetime\", \"dropoff_datetime\",            # Times\n",
    "        \"pickup_latitude\", \"pickup_longitude\",            # Pickup coordinates\n",
    "        \"dropoff_latitude\", \"dropoff_longitude\",          # Dropoff coordinates\n",
    "        \"hvfhs_license_num\", \"trip_miles\",\n",
    "        \"base_passenger_fare\", \"tolls\", \"tips\",\t\n",
    "        \"sales_tax\", \"congestion_surcharge\"\n",
    "    ]\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean HVFHV data for Uber rides\n",
    "print(\"Cleaning HVFHV (Uber) data...\")\n",
    "cleaned_fhv = clean_fhv_data(sampled_data_HVFHV, zone_centers)\n",
    "\n",
    "# Save cleaned HVFHV data\n",
    "cleaned_fhv.to_parquet(\"hvfhv_cleaned.parquet\")\n",
    "print(\"Cleaned HVFHV data saved: hvfhv_cleaned.parquet\")\n",
    "\n",
    "cleaned_fhv.head()\n",
    "cleaned_fhv.info()\n",
    "cleaned_fhv.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Loading and cleaning weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\3625293923.py:6: DtypeWarning: Columns (8,9,10,17,18,64,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\3625293923.py:6: DtypeWarning: Columns (9,10,41,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\3625293923.py:6: DtypeWarning: Columns (8,10,17,18,41,62,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\3625293923.py:6: DtypeWarning: Columns (10,13,15,20,41,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\3625293923.py:6: DtypeWarning: Columns (8,9,10,15,17,18,19,20,38,41,42,43,44,58,64,65,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>REPORT_TYPE</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>HourlyAltimeterSetting</th>\n",
       "      <th>HourlyDewPointTemperature</th>\n",
       "      <th>...</th>\n",
       "      <th>BackupDirection</th>\n",
       "      <th>BackupDistance</th>\n",
       "      <th>BackupDistanceUnit</th>\n",
       "      <th>BackupElements</th>\n",
       "      <th>BackupElevation</th>\n",
       "      <th>BackupEquipment</th>\n",
       "      <th>BackupLatitude</th>\n",
       "      <th>BackupLongitude</th>\n",
       "      <th>BackupName</th>\n",
       "      <th>WindEquipmentChangeDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2020-01-01T00:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.66</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...</td>\n",
       "      <td>2006-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2020-01-01T01:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.67</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...</td>\n",
       "      <td>2006-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2020-01-01T02:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.68</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...</td>\n",
       "      <td>2006-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2020-01-01T03:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.70</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...</td>\n",
       "      <td>2006-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72505394728</td>\n",
       "      <td>2020-01-01T04:51:00</td>\n",
       "      <td>40.77898</td>\n",
       "      <td>-73.96925</td>\n",
       "      <td>42.7</td>\n",
       "      <td>NY CITY CENTRAL PARK, NY US</td>\n",
       "      <td>FM-15</td>\n",
       "      <td>7</td>\n",
       "      <td>29.70</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SNOWBOARD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...</td>\n",
       "      <td>2006-09-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATION                 DATE  LATITUDE  LONGITUDE  ELEVATION  \\\n",
       "0  72505394728  2020-01-01T00:51:00  40.77898  -73.96925       42.7   \n",
       "1  72505394728  2020-01-01T01:51:00  40.77898  -73.96925       42.7   \n",
       "2  72505394728  2020-01-01T02:51:00  40.77898  -73.96925       42.7   \n",
       "3  72505394728  2020-01-01T03:51:00  40.77898  -73.96925       42.7   \n",
       "4  72505394728  2020-01-01T04:51:00  40.77898  -73.96925       42.7   \n",
       "\n",
       "                          NAME REPORT_TYPE  SOURCE HourlyAltimeterSetting  \\\n",
       "0  NY CITY CENTRAL PARK, NY US       FM-15       7                  29.66   \n",
       "1  NY CITY CENTRAL PARK, NY US       FM-15       7                  29.67   \n",
       "2  NY CITY CENTRAL PARK, NY US       FM-15       7                  29.68   \n",
       "3  NY CITY CENTRAL PARK, NY US       FM-15       7                  29.70   \n",
       "4  NY CITY CENTRAL PARK, NY US       FM-15       7                  29.70   \n",
       "\n",
       "  HourlyDewPointTemperature  ... BackupDirection BackupDistance  \\\n",
       "0                        26  ...             NaN            NaN   \n",
       "1                        27  ...             NaN            NaN   \n",
       "2                        26  ...             NaN            NaN   \n",
       "3                        24  ...             NaN            NaN   \n",
       "4                        23  ...             NaN            NaN   \n",
       "\n",
       "  BackupDistanceUnit BackupElements  BackupElevation BackupEquipment  \\\n",
       "0                NaN           SNOW              NaN       SNOWBOARD   \n",
       "1                NaN           SNOW              NaN       SNOWBOARD   \n",
       "2                NaN           SNOW              NaN       SNOWBOARD   \n",
       "3                NaN           SNOW              NaN       SNOWBOARD   \n",
       "4                NaN           SNOW              NaN       SNOWBOARD   \n",
       "\n",
       "  BackupLatitude BackupLongitude  \\\n",
       "0            NaN             NaN   \n",
       "1            NaN             NaN   \n",
       "2            NaN             NaN   \n",
       "3            NaN             NaN   \n",
       "4            NaN             NaN   \n",
       "\n",
       "                                          BackupName WindEquipmentChangeDate  \n",
       "0  CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...              2006-09-18  \n",
       "1  CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...              2006-09-18  \n",
       "2  CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...              2006-09-18  \n",
       "3  CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...              2006-09-18  \n",
       "4  CENTRAL PARK CONSERVANCY AT 79TH ST TRANSVERSE...              2006-09-18  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = r\"E:\\2024 Fall Academic\\Tools for Analytics\\weather data\"\n",
    "weather_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "dataframes = []\n",
    "for weather_file in weather_files:\n",
    "    file_path = os.path.join(folder_path, weather_file)\n",
    "    df = pd.read_csv(file_path)  \n",
    "    dataframes.append(df) \n",
    "merged_weather_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56098 entries, 0 to 56097\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   Report_type        56098 non-null  object        \n",
      " 1   observation_date   56098 non-null  datetime64[ns]\n",
      " 2   geo_latitude       56098 non-null  float64       \n",
      " 3   geo_longitude      56098 non-null  float64       \n",
      " 4   hourly_precip_mm   56098 non-null  float64       \n",
      " 5   hourly_wind_mps    56098 non-null  float64       \n",
      " 6   daily_snowfall_mm  56098 non-null  float64       \n",
      " 7   daily_precip_mm    56098 non-null  float64       \n",
      " 8   daily_wind_mps     56098 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(7), object(1)\n",
      "memory usage: 3.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data.rename(columns={\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['observation_date'] = pd.to_datetime(weather_data['observation_date'])\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data[columns_to_fill] = weather_data[columns_to_fill].fillna(0)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['Report_type'] = weather_data['Report_type'].astype(object)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['hourly_precip_mm'] = weather_data['hourly_precip_mm'].astype(float)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['daily_precip_mm'] = weather_data['daily_precip_mm'].astype(float)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['hourly_wind_mps'] = weather_data['hourly_wind_mps'].astype(float)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['daily_wind_mps'] = weather_data['daily_wind_mps'].astype(float)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['daily_snowfall_mm'] = weather_data['daily_snowfall_mm'].astype(float)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['geo_latitude'] = weather_data['geo_latitude'].astype(float)\n",
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1549190904.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  weather_data['geo_longitude'] = weather_data['geo_longitude'].astype(float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>observation_date</th>\n",
       "      <th>geo_latitude</th>\n",
       "      <th>geo_longitude</th>\n",
       "      <th>hourly_precip_mm</th>\n",
       "      <th>hourly_wind_mps</th>\n",
       "      <th>daily_snowfall_mm</th>\n",
       "      <th>daily_precip_mm</th>\n",
       "      <th>daily_wind_mps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56098</td>\n",
       "      <td>5.609800e+04</td>\n",
       "      <td>5.609800e+04</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-29 21:14:19.618881024</td>\n",
       "      <td>4.077898e+01</td>\n",
       "      <td>-7.396925e+01</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>4.537238</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.151276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>4.077898e+01</td>\n",
       "      <td>-7.396925e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-18 19:01:45</td>\n",
       "      <td>4.077898e+01</td>\n",
       "      <td>-7.396925e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-28 01:21:00</td>\n",
       "      <td>4.077898e+01</td>\n",
       "      <td>-7.396925e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-15 05:39:00</td>\n",
       "      <td>4.077898e+01</td>\n",
       "      <td>-7.396925e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>4.077898e+01</td>\n",
       "      <td>-7.396925e+01</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>2237.000000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>7.130000</td>\n",
       "      <td>14.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.214267e-11</td>\n",
       "      <td>5.815134e-11</td>\n",
       "      <td>0.056783</td>\n",
       "      <td>13.883208</td>\n",
       "      <td>0.087521</td>\n",
       "      <td>0.077361</td>\n",
       "      <td>0.948191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    observation_date  geo_latitude  geo_longitude  \\\n",
       "count                          56098  5.609800e+04   5.609800e+04   \n",
       "mean   2022-05-29 21:14:19.618881024  4.077898e+01  -7.396925e+01   \n",
       "min              2020-01-01 00:51:00  4.077898e+01  -7.396925e+01   \n",
       "25%              2021-03-18 19:01:45  4.077898e+01  -7.396925e+01   \n",
       "50%              2022-05-28 01:21:00  4.077898e+01  -7.396925e+01   \n",
       "75%              2023-08-15 05:39:00  4.077898e+01  -7.396925e+01   \n",
       "max              2024-10-22 18:51:00  4.077898e+01  -7.396925e+01   \n",
       "std                              NaN  4.214267e-11   5.815134e-11   \n",
       "\n",
       "       hourly_precip_mm  hourly_wind_mps  daily_snowfall_mm  daily_precip_mm  \\\n",
       "count      56098.000000     56098.000000       56098.000000     56098.000000   \n",
       "mean           0.010511         4.537238           0.001223         0.004441   \n",
       "min            0.000000         0.000000           0.000000         0.000000   \n",
       "25%            0.000000         0.000000           0.000000         0.000000   \n",
       "50%            0.000000         5.000000           0.000000         0.000000   \n",
       "75%            0.000000         7.000000           0.000000         0.000000   \n",
       "max            3.470000      2237.000000          14.800000         7.130000   \n",
       "std            0.056783        13.883208           0.087521         0.077361   \n",
       "\n",
       "       daily_wind_mps  \n",
       "count    56098.000000  \n",
       "mean         0.151276  \n",
       "min          0.000000  \n",
       "25%          0.000000  \n",
       "50%          0.000000  \n",
       "75%          0.000000  \n",
       "max         14.200000  \n",
       "std          0.948191  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the mapping for column data types\n",
    "dtype_mapping = {\n",
    "    \"HourlyPrecipitation\": \"string\",\n",
    "    \"DailySnowfall\": \"string\",\n",
    "    \"DailyPrecipitation\": \"string\",\n",
    "    \"LATITUDE\": \"float\",\n",
    "    \"LONGITUDE\": \"float\",\n",
    "}\n",
    "\n",
    "# Load the dataset and specify data types\n",
    "# weather_filepath = r\"D:\\Columbia University\\24 Fall\\IEOR 4501\\Final Project\\merged_weather_df.csv\"\n",
    "weather_data = merged_weather_df\n",
    "\n",
    "# Convert specific columns to numeric types, handling mixed types automatically\n",
    "numeric_columns = [\"HourlyPrecipitation\", \"DailySnowfall\", \"DailyPrecipitation\"]\n",
    "for col in numeric_columns:\n",
    "    # Convert column to string first to handle mixed or non-string values\n",
    "    weather_data[col] = weather_data[col].astype(str)\n",
    "    weather_data[col] = (\n",
    "        weather_data[col]\n",
    "        .str.extract(r'([\\d\\.]+)', expand=False)  # Extract numeric parts\n",
    "        .astype(float)\n",
    "        .fillna(0)  # Fill missing values with 0\n",
    "    )\n",
    "    \n",
    "\n",
    "# Retain the required columns\n",
    "weather_columns_to_keep = [\"REPORT_TYPE\", \"DATE\", \"LATITUDE\", \"LONGITUDE\", \"HourlyPrecipitation\", \n",
    "                           \"HourlyWindSpeed\", \"DailySnowfall\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\"]\n",
    "weather_data = weather_data[weather_columns_to_keep]\n",
    "\n",
    "# Rename columns to maintain consistency\n",
    "weather_data.rename(columns={\n",
    "    \"REPORT_TYPE\": \"Report_type\",\n",
    "    \"DATE\": \"observation_date\",\n",
    "    \"LATITUDE\": \"geo_latitude\",\n",
    "    \"LONGITUDE\": \"geo_longitude\",\n",
    "    \"HourlyPrecipitation\": \"hourly_precip_mm\",\n",
    "    \"DailyPrecipitation\": \"daily_precip_mm\",\n",
    "    \"HourlyWindSpeed\": \"hourly_wind_mps\",\n",
    "    \"DailyAverageWindSpeed\": \"daily_wind_mps\",\n",
    "    \"DailySnowfall\": \"daily_snowfall_mm\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert the date column to datetime type\n",
    "weather_data['observation_date'] = pd.to_datetime(weather_data['observation_date'])\n",
    "\n",
    "# Fill missing values with 0 (for specific columns only)\n",
    "columns_to_fill = ['hourly_precip_mm', 'daily_precip_mm', \n",
    "                   'hourly_wind_mps', 'daily_wind_mps', 'daily_snowfall_mm']\n",
    "weather_data[columns_to_fill] = weather_data[columns_to_fill].fillna(0)\n",
    "\n",
    "# Ensure the data types of columns are correct\n",
    "weather_data['Report_type'] = weather_data['Report_type'].astype(object)\n",
    "weather_data['hourly_precip_mm'] = weather_data['hourly_precip_mm'].astype(float)\n",
    "weather_data['daily_precip_mm'] = weather_data['daily_precip_mm'].astype(float)\n",
    "weather_data['hourly_wind_mps'] = weather_data['hourly_wind_mps'].astype(float)\n",
    "weather_data['daily_wind_mps'] = weather_data['daily_wind_mps'].astype(float)\n",
    "weather_data['daily_snowfall_mm'] = weather_data['daily_snowfall_mm'].astype(float)\n",
    "weather_data['geo_latitude'] = weather_data['geo_latitude'].astype(float)\n",
    "weather_data['geo_longitude'] = weather_data['geo_longitude'].astype(float)\n",
    "\n",
    "# Save the cleaned data to a file\n",
    "weather_output_file = \"cleaned_weather_data.parquet\"\n",
    "weather_data.to_parquet(weather_output_file, index=False)\n",
    "\n",
    "weather_data = pd.read_parquet(weather_output_file)\n",
    "weather_data.head()\n",
    "weather_data.info()\n",
    "weather_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering hourly weather data\n",
    "weather_data['Report_type'] = weather_data['Report_type'].str.strip()\n",
    "hourly_weather_df = weather_data[weather_data[\"Report_type\"] == \"FM-15\"]\n",
    "hourly_weather_df = hourly_weather_df.dropna(axis=1, how='all')\n",
    "hourly_weather_df.head()\n",
    "hourly_weather_df.to_csv(\"hourly_weather_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering daily weather data\n",
    "daily_weather_df = weather_data[weather_data[\"Report_type\"] == \"SOD\"]\n",
    "daily_weather_df = daily_weather_df.dropna(axis=1, how='all')\n",
    "daily_weather_df.head()\n",
    "# Save the daily_weather_df to a CSV file\n",
    "daily_weather_df.to_csv(\"daily_weather_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, Float, String, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\perki\\AppData\\Local\\Temp\\ipykernel_5328\\1409344703.py:13: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully.\n",
      "Populated table: yellow_taxi\n",
      "Populated table: hvfhv\n",
      "Populated table: hourly_weather\n",
      "Populated table: daily_weather\n",
      "Schema file generated: schema.sql\n",
      "SQLite database setup and population complete.\n"
     ]
    }
   ],
   "source": [
    "# Paths to cleaned datasets\n",
    "yellow_taxi_file = \"yellow_taxi_cleaned.parquet\"\n",
    "hvfhv_file = \"hvfhv_cleaned.parquet\"\n",
    "hourly_weather_file = \"hourly_weather_cleaned.csv\"\n",
    "daily_weather_file = \"daily_weather_cleaned.csv\"\n",
    "\n",
    "# SQLite Database Name\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "database_name = \"project.db\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(f\"sqlite:///{database_name}\")\n",
    "Base = declarative_base()\n",
    "\n",
    "# Step 1: Define Tables Using SQLAlchemy Models\n",
    "class YellowTaxi(Base):\n",
    "    __tablename__ = \"yellow_taxi\"\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    tpep_pickup_datetime = Column(Text)\n",
    "    tpep_dropoff_datetime = Column(Text)\n",
    "    passenger_count = Column(Integer)\n",
    "    trip_distance = Column(Float)\n",
    "    fare_amount = Column(Float)\n",
    "    total_amount = Column(Float)\n",
    "    payment_type = Column(Integer)\n",
    "    ratecodeid = Column(Float)\n",
    "    store_and_fwd_flag = Column(String)\n",
    "    pickup_latitude = Column(Float)\n",
    "    pickup_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "\n",
    "class HVFHV(Base):\n",
    "    __tablename__ = \"hvfhv\"\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    pickup_datetime = Column(Text)\n",
    "    dropoff_datetime = Column(Text)\n",
    "    pickup_latitude = Column(Float)\n",
    "    pickup_longitude = Column(Float)\n",
    "    dropoff_latitude = Column(Float)\n",
    "    dropoff_longitude = Column(Float)\n",
    "    hvfhs_license_num = Column(String)\n",
    "    trip_miles = Column(Float)\n",
    "\n",
    "class HourlyWeather(Base):\n",
    "    __tablename__ = \"hourly_weather\"\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    observation_date = Column(Text)  # Matches the date column in your hourly weather dataset\n",
    "    geo_latitude = Column(Float)\n",
    "    geo_longitude = Column(Float)\n",
    "    hourly_precip_mm = Column(Float)\n",
    "    hourly_wind_mps = Column(Float)\n",
    "\n",
    "class DailyWeather(Base):\n",
    "    __tablename__ = \"daily_weather\"\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    observation_date = Column(Text)  # Matches the date column in your daily weather dataset\n",
    "    geo_latitude = Column(Float)\n",
    "    geo_longitude = Column(Float)\n",
    "    daily_precip_mm = Column(Float)\n",
    "    daily_snowfall_mm = Column(Float)\n",
    "    daily_wind_mps = Column(Float)\n",
    "\n",
    "# Create tables in the database\n",
    "Base.metadata.create_all(engine)\n",
    "print(\"Tables created successfully.\")\n",
    "\n",
    "# Step 2: Populate Tables with Data\n",
    "# Load datasets\n",
    "yellow_taxi_data = pd.read_parquet(yellow_taxi_file)\n",
    "hvfhv_data = pd.read_parquet(hvfhv_file)\n",
    "hourly_weather_data = pd.read_csv(hourly_weather_file)\n",
    "daily_weather_data = pd.read_csv(daily_weather_file)\n",
    "\n",
    "# Write data to the database\n",
    "yellow_taxi_data.to_sql(\"yellow_taxi\", engine, if_exists=\"replace\", index=False)\n",
    "print(\"Populated table: yellow_taxi\")\n",
    "\n",
    "hvfhv_data.to_sql(\"hvfhv\", engine, if_exists=\"replace\", index=False)\n",
    "print(\"Populated table: hvfhv\")\n",
    "\n",
    "hourly_weather_data.to_sql(\"hourly_weather\", engine, if_exists=\"replace\", index=False)\n",
    "print(\"Populated table: hourly_weather\")\n",
    "\n",
    "daily_weather_data.to_sql(\"daily_weather\", engine, if_exists=\"replace\", index=False)\n",
    "print(\"Populated table: daily_weather\")\n",
    "\n",
    "# Step 3: Generate schema.sql File\n",
    "schema_file = \"schema.sql\"\n",
    "with open(schema_file, \"w\") as f:\n",
    "    for table in Base.metadata.tables.values():\n",
    "        f.write(str(table.compile(engine)) + \";\\n\\n\")\n",
    "print(f\"Schema file generated: {schema_file}\")\n",
    "\n",
    "print(\"SQLite database setup and population complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "yellow_taxi\n",
      "hvfhv\n",
      "hourly_weather\n",
      "daily_weather\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(\"project.db\")\n",
    "\n",
    "# List all tables in the database\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.sql import text\n",
    "import pandas as pd\n",
    "\n",
    "# SQLite database file\n",
    "database_name = \"project.db\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f\"sqlite:///{database_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"\"\"\n",
    "SELECT\n",
    "    strftime('%H', tpep_pickup_datetime) AS pickup_hour,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM\n",
    "    yellow_taxi\n",
    "GROUP BY\n",
    "    pickup_hour\n",
    "ORDER BY\n",
    "    ride_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "with open(\"Most_popular_taxi_hours.sql\", \"w\") as file:\n",
    "    file.write(query_1)\n",
    "    \n",
    "# Execute the query and load results into a DataFrame\n",
    "with engine.connect() as connection:\n",
    "    results = pd.read_sql(query_1, connection)\n",
    "\n",
    "\n",
    "# Display the results\n",
    "print(\"Most popular hours to take a taxi:\")\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = \"\"\"\n",
    "SELECT\n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM\n",
    "    hvfhv\n",
    "WHERE\n",
    "    hvfhs_license_num = 'HV0003'\n",
    "GROUP BY\n",
    "    day_of_week\n",
    "ORDER BY\n",
    "    ride_count DESC;\n",
    "\"\"\"\n",
    "\n",
    "with open(\"uber_popular_day_results.sql\", \"w\") as file:\n",
    "    file.write(query_2)\n",
    "    \n",
    "# Execute the query and load results into a DataFrame\n",
    "with engine.connect() as connection:\n",
    "    uber_popular_day_results = pd.read_sql(query_2, connection)\n",
    "\n",
    "# Replace numeric day values with weekday names\n",
    "weekday_mapping = {\n",
    "    '0': 'Sunday',\n",
    "    '1': 'Monday',\n",
    "    '2': 'Tuesday',\n",
    "    '3': 'Wednesday',\n",
    "    '4': 'Thursday',\n",
    "    '5': 'Friday',\n",
    "    '6': 'Saturday'\n",
    "}\n",
    "uber_popular_day_results['day_of_week'] = uber_popular_day_results['day_of_week'].map(weekday_mapping)\n",
    "\n",
    "print(\"Most popular days to take an Uber:\")\n",
    "uber_popular_day_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get trip distances (miles) for January 2024\n",
    "query_3 = \"\"\"\n",
    "SELECT trip_distance FROM yellow_taxi\n",
    "WHERE strftime('%Y-%m', tpep_pickup_datetime) = '2024-01'\n",
    "UNION ALL\n",
    "SELECT trip_miles AS trip_distance FROM hvfhv\n",
    "WHERE strftime('%Y-%m', pickup_datetime) = '2024-01';\n",
    "\"\"\"\n",
    "# Write the query to a .sql file for reference\n",
    "with open(\"trip_distance_95_percentile.sql\", \"w\") as file:\n",
    "    file.write(query_3)\n",
    "\n",
    "# Execute the query\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(query_3)).fetchone()\n",
    "\n",
    "# Display the result\n",
    "if result:\n",
    "    print(\"95% Percentile of Trip Distance in January 2024:\", result[0])\n",
    "else:\n",
    "    print(\"No data available for January 2024.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: Busiest days in 2023 with weather data\n",
    "query_4 = \"\"\"\n",
    "WITH daily_rides AS (\n",
    "    SELECT\n",
    "        strftime('%Y-%m-%d', tpep_pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_distance) AS avg_distance\n",
    "    FROM yellow_taxi\n",
    "    WHERE strftime('%Y', tpep_pickup_datetime) = '2023'\n",
    "    GROUP BY ride_date\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT\n",
    "        strftime('%Y-%m-%d', pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_miles) AS avg_distance\n",
    "    FROM hvfhv\n",
    "    WHERE strftime('%Y', pickup_datetime) = '2023'\n",
    "    GROUP BY ride_date\n",
    "),\n",
    "combined_rides AS (\n",
    "    SELECT\n",
    "        ride_date,\n",
    "        SUM(total_rides) AS total_rides,\n",
    "        AVG(avg_distance) AS avg_distance\n",
    "    FROM daily_rides\n",
    "    GROUP BY ride_date\n",
    "),\n",
    "busiest_days AS (\n",
    "    SELECT \n",
    "        ride_date,\n",
    "        total_rides,\n",
    "        avg_distance\n",
    "    FROM combined_rides\n",
    "    ORDER BY total_rides DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT \n",
    "    b.ride_date,\n",
    "    b.total_rides,\n",
    "    b.avg_distance,\n",
    "    AVG(w.daily_precip_mm) AS avg_precipitation,\n",
    "    AVG(w.daily_wind_mps) AS avg_wind_speed\n",
    "FROM busiest_days b\n",
    "LEFT JOIN daily_weather w\n",
    "ON b.ride_date = strftime('%Y-%m-%d', w.observation_date)\n",
    "GROUP BY b.ride_date\n",
    "ORDER BY b.total_rides DESC;\n",
    "\"\"\"\n",
    "\n",
    "# Write the query to a .sql file for reference\n",
    "with open(\"Busiest_days_2023.sql\", \"w\") as file:\n",
    "    file.write(query_4)\n",
    "\n",
    "# Execute the query and fetch results\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(query_4)).fetchall()\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "columns = [\"date\", \"total_rides\", \"avg_distance\", \"avg_precipitation\", \"avg_wind_speed\"]\n",
    "busiest_days_df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "# Display the result\n",
    "busiest_days_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Snowiest days with hired rides\n",
    "query_5 = \"\"\"\n",
    "WITH daily_snow_trips AS (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d', tpep_pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM yellow_taxi\n",
    "    WHERE tpep_pickup_datetime BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "    GROUP BY ride_date\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d', pickup_datetime) AS ride_date,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM hvfhv\n",
    "    WHERE pickup_datetime BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "    GROUP BY ride_date\n",
    "),\n",
    "combined_snow_trips AS (\n",
    "    SELECT \n",
    "        ride_date,\n",
    "        SUM(total_rides) AS total_rides\n",
    "    FROM daily_snow_trips\n",
    "    GROUP BY ride_date\n",
    "),\n",
    "snowiest_days AS (\n",
    "    SELECT \n",
    "        strftime('%Y-%m-%d', observation_date) AS snow_date,\n",
    "        SUM(daily_snowfall_mm) AS total_snowfall\n",
    "    FROM daily_weather\n",
    "    WHERE observation_date BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "    GROUP BY snow_date\n",
    ")\n",
    "SELECT \n",
    "    s.snow_date,\n",
    "    s.total_snowfall,\n",
    "    COALESCE(t.total_rides, 0) AS total_rides\n",
    "FROM snowiest_days s\n",
    "LEFT JOIN combined_snow_trips t\n",
    "ON s.snow_date = t.ride_date\n",
    "ORDER BY s.total_snowfall DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "# Write the query to a .sql file for reference\n",
    "with open(\"Snowiest_days_hired_rides.sql\", \"w\") as file:\n",
    "    file.write(query_5)\n",
    "    \n",
    "# Execute the query and fetch results\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(query_5)).fetchall()\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "columns = [\"date\", \"total_snowfall\", \"total_rides\"]\n",
    "snowiest_days_df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "# Display the result\n",
    "snowiest_days_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_6 = \"\"\"\n",
    "WITH all_hired_rides_hourly AS (\n",
    "    SELECT \n",
    "        STRFTIME('%Y-%m-%d %H:00:00', tpep_pickup_datetime) AS hour,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM yellow_taxi\n",
    "    WHERE DATE(tpep_pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY hour\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        STRFTIME('%Y-%m-%d %H:00:00', pickup_datetime) AS hour,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM hvfhv\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY hour\n",
    "),\n",
    "weather_hourly AS (\n",
    "    SELECT \n",
    "        STRFTIME('%Y-%m-%d %H:00:00', observation_date) AS hour,\n",
    "        SUM(hourly_precip_mm) AS total_precipitation,\n",
    "        AVG(hourly_wind_mps) AS avg_wind_speed\n",
    "    FROM hourly_weather\n",
    "    WHERE DATE(observation_date) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY hour\n",
    "),\n",
    "all_hours AS (\n",
    "    SELECT DISTINCT\n",
    "        STRFTIME('%Y-%m-%d %H:00:00', observation_date) AS hour\n",
    "    FROM hourly_weather\n",
    "    WHERE DATE(observation_date) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    UNION\n",
    "    SELECT DISTINCT\n",
    "        STRFTIME('%Y-%m-%d %H:00:00', tpep_pickup_datetime) AS hour\n",
    "    FROM yellow_taxi\n",
    "    WHERE DATE(tpep_pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    UNION\n",
    "    SELECT DISTINCT\n",
    "        STRFTIME('%Y-%m-%d %H:00:00', pickup_datetime) AS hour\n",
    "    FROM hvfhv\n",
    "    WHERE DATE(pickup_datetime) BETWEEN '2023-09-25' AND '2023-10-03'\n",
    ")\n",
    "SELECT \n",
    "    h.hour,\n",
    "    COALESCE(r.total_rides, 0) AS total_rides,\n",
    "    COALESCE(w.total_precipitation, 0.0) AS total_precipitation,\n",
    "    COALESCE(w.avg_wind_speed, 0.0) AS avg_wind_speed\n",
    "FROM all_hours h\n",
    "LEFT JOIN all_hired_rides_hourly r ON h.hour = r.hour\n",
    "LEFT JOIN weather_hourly w ON h.hour = w.hour\n",
    "ORDER BY h.hour ASC;\n",
    "\"\"\"\n",
    "\n",
    "# Execute Query 6\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(query_6)).fetchall()\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "columns = [\"hour\", \"total_rides\", \"total_precipitation\", \"avg_wind_speed\"]\n",
    "ophelia_hourly_data_df = pd.DataFrame(result, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "ophelia_hourly_data_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
